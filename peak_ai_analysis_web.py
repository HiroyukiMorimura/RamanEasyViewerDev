# -*- coding: utf-8 -*-
"""
ãƒ”ãƒ¼ã‚¯AIè§£æãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
RAGæ©Ÿèƒ½ã¨OpenAI APIã‚’ä½¿ç”¨ã—ãŸãƒ©ãƒãƒ³ã‚¹ãƒšã‚¯ãƒˆãƒ«ã®é«˜åº¦ãªè§£æ
Enhanced with comprehensive features and PDF report generation

Created on Wed Jun 11 15:56:04 2025
@author: Enhanced System
"""

import streamlit as st
import numpy as np
import pandas as pd
import time
import os
import json
import pickle
import requests
import ssl
import urllib3
import glob
import warnings
from datetime import datetime
from typing import List, Dict, Optional
import plotly.graph_objects as go
from plotly.subplots import make_subplots
from scipy.signal import savgol_filter, find_peaks, peak_prominences
from pathlib import Path
from common_utils import *
from peak_analysis_web import optimize_thresholds_via_gridsearch

# è­¦å‘Šã‚’æŠ‘åˆ¶
warnings.filterwarnings('ignore', category=UserWarning, module='scipy.signal._peak_finding')

# ã‚·ã‚¹ãƒ†ãƒ ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
def handle_system_warnings():
    """ã‚·ã‚¹ãƒ†ãƒ è­¦å‘Šã¨ã‚¨ãƒ©ãƒ¼ã®ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°"""
    try:
        # inotifyåˆ¶é™ã®ç¢ºèª
        import subprocess
        result = subprocess.run(['cat', '/proc/sys/fs/inotify/max_user_instances'], 
                              capture_output=True, text=True, timeout=2)
        if result.returncode == 0:
            max_instances = int(result.stdout.strip())
            if max_instances < 512:
                st.sidebar.warning(f"âš ï¸ ã‚·ã‚¹ãƒ†ãƒ åˆ¶é™: inotify instances = {max_instances}. "
                                 f"ãƒ•ã‚¡ã‚¤ãƒ«ç›£è¦–æ©Ÿèƒ½ãŒåˆ¶é™ã•ã‚Œã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚")
    except:
        pass  # ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±å–å¾—å¤±æ•—ã¯ç„¡è¦–

# Streamlitè¨­å®šã®æœ€é©åŒ–ï¼ˆinotifyåˆ¶é™å¯¾ç­–ï¼‰
if hasattr(st, '_config'):
    try:
        # ãƒ•ã‚¡ã‚¤ãƒ«ç›£è¦–ã‚’æœ€å°é™ã«æŠ‘åˆ¶
        st._config.set_option('server.fileWatcherType', 'none')
        st._config.set_option('server.runOnSave', False)
    except:
        pass  # è¨­å®šå¤‰æ›´ã«å¤±æ•—ã—ã¦ã‚‚ç¶šè¡Œ

# ç’°å¢ƒå¤‰æ•°ã§ã®Streamlitè¨­å®šï¼ˆã‚·ã‚¹ãƒ†ãƒ ãƒ¬ãƒ™ãƒ«å¯¾ç­–ã®ææ¡ˆï¼‰
def suggest_system_optimization():
    """ã‚·ã‚¹ãƒ†ãƒ æœ€é©åŒ–ã®ææ¡ˆã‚’è¡¨ç¤º"""
    if os.path.exists('/proc/sys/fs/inotify/max_user_instances'):
        with st.sidebar.expander("âš™ï¸ ã‚·ã‚¹ãƒ†ãƒ æœ€é©åŒ–ã®ãƒ’ãƒ³ãƒˆ", expanded=False):
            st.markdown("""
            **Linuxç’°å¢ƒã§ã®inotifyåˆ¶é™å¯¾ç­–:**
            
            ```bash
            # ä¸€æ™‚çš„ãªå¢—åŠ 
            echo 512 | sudo tee /proc/sys/fs/inotify/max_user_instances
            
            # æ°¸ç¶šçš„ãªè¨­å®š
            echo 'fs.inotify.max_user_instances=512' | sudo tee -a /etc/sysctl.conf
            sudo sysctl -p
            ```
            
            **Streamlitç’°å¢ƒå¤‰æ•°è¨­å®š:**
            ```bash
            export STREAMLIT_SERVER_FILE_WATCHER_TYPE=none
            export STREAMLIT_SERVER_RUN_ON_SAVE=false
            ```
            """)

# ã‚·ã‚¹ãƒ†ãƒ æœ€é©åŒ–ææ¡ˆã‚’è¡¨ç¤º
suggest_system_optimization()

# PDFç”Ÿæˆé–¢é€£ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼ˆæ–°æ©Ÿèƒ½ï¼‰
try:
    from reportlab.lib.pagesizes import letter, A4
    from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Image, Table, TableStyle, PageBreak
    from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle
    from reportlab.lib.units import inch, cm
    from reportlab.lib import colors
    from reportlab.lib.enums import TA_CENTER, TA_LEFT, TA_JUSTIFY
    from reportlab.pdfbase import pdfmetrics
    from reportlab.pdfbase.ttfonts import TTFont
    from reportlab.pdfbase import pdfmetrics
    from reportlab.pdfbase.cidfonts import UnicodeCIDFont
    from PIL import Image as PILImage
    import plotly.io as pio
    PDF_GENERATION_AVAILABLE = True
except ImportError as e:
    PDF_GENERATION_AVAILABLE = False
    st.warning("PDFãƒ¬ãƒãƒ¼ãƒˆæ©Ÿèƒ½ã‚’ä½¿ç”¨ã™ã‚‹ã«ã¯ã€reportlab, Pillowãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒå¿…è¦ã§ã™")

# ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    from security_manager import (
        SecurityManager, 
        get_security_manager, 
        SecurityConfig, 
        SecurityException
    )
    SECURITY_AVAILABLE = True
except ImportError:
    SECURITY_AVAILABLE = False
    st.warning("ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚åŸºæœ¬æ©Ÿèƒ½ã®ã¿å‹•ä½œã—ã¾ã™ã€‚")

# Interactive plotting
try:
    from streamlit_plotly_events import plotly_events
except ImportError:
    plotly_events = None

# AI/RAGé–¢é€£ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    import PyPDF2
    import docx
    import openai
    import faiss
    from sentence_transformers import SentenceTransformer
    PDF_AVAILABLE = True
except ImportError:
    PDF_AVAILABLE = False
    st.warning("AI analysis features require additional packages: PyPDF2, docx, openai, faiss, sentence-transformers")

# OpenAI API Keyï¼ˆç’°å¢ƒå¤‰æ•°ã‹ã‚‰å–å¾—ã‚’æ¨å¥¨ï¼‰
openai_api_key = st.secrets["openai"]["openai_api_key"]

def check_internet_connection():
    """ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆæ¥ç¶šãƒã‚§ãƒƒã‚¯"""
    try:
        # HTTPSæ¥ç¶šã®ã¿ã‚’è¨±å¯
        response = requests.get("https://www.google.com", timeout=5, verify=True)
        return response.status_code == 200
    except requests.exceptions.RequestException:
        return False

def setup_ssl_context():
    """SSLã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®è¨­å®š"""
    try:
        # SSLè¨¼æ˜æ›¸æ¤œè¨¼ã‚’å¼·åˆ¶
        ssl_context = ssl.create_default_context()
        ssl_context.check_hostname = True
        ssl_context.verify_mode = ssl.CERT_REQUIRED
        
        # TLS 1.2ä»¥ä¸Šã‚’å¼·åˆ¶
        ssl_context.minimum_version = ssl.TLSVersion.TLSv1_2
        
        # å¼±ã„æš—å·åŒ–ã‚’ç„¡åŠ¹åŒ–
        ssl_context.set_ciphers('ECDHE+AESGCM:ECDHE+CHACHA20:DHE+AESGCM:DHE+CHACHA20:!aNULL:!MD5:!DSS')
        
        return ssl_context
    except Exception as e:
        st.error(f"SSLè¨­å®šã‚¨ãƒ©ãƒ¼: {e}")
        return None

def save_original_spectrum_data_to_session(result, file_key):
    """å…ƒã®ã‚¹ãƒšã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’ã‚»ãƒƒã‚·ãƒ§ãƒ³ã«ä¿å­˜"""
    try:
        # æœ‰åŠ¹ãªãƒ”ãƒ¼ã‚¯ã‚’è¨ˆç®—
        filtered_peaks = [
            i for i in result["detected_peaks"]
            if i not in st.session_state.get(f"{file_key}_excluded_peaks", set())
        ]
        
        # å…ƒãƒ‡ãƒ¼ã‚¿ã‚’è¾æ›¸å½¢å¼ã§ä¿å­˜
        original_data = {
            'wavenum': result['wavenum'].tolist() if hasattr(result['wavenum'], 'tolist') else list(result['wavenum']),
            'spectrum': result['spectrum'].tolist() if hasattr(result['spectrum'], 'tolist') else list(result['spectrum']),
            'second_derivative': result['second_derivative'].tolist() if hasattr(result['second_derivative'], 'tolist') else list(result['second_derivative']),
            'detected_peaks': list(result['detected_peaks']),
            'detected_prominences': list(result['detected_prominences']),
            'manual_peaks': st.session_state.get(f"{file_key}_manual_peaks", []),
            'excluded_peaks': st.session_state.get(f"{file_key}_excluded_peaks", set()),
            'all_peaks': list(result.get('all_peaks', [])),
            'all_prominences': list(result.get('all_prominences', [])),
            'filtered_peaks': filtered_peaks
        }
        
        st.session_state[f"{file_key}_original_spectrum_data"] = original_data
        st.success("âœ… å…ƒã®ã‚¹ãƒšã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜ã—ã¾ã—ãŸ")
        
    except Exception as e:
        st.warning(f"å…ƒãƒ‡ãƒ¼ã‚¿ä¿å­˜è­¦å‘Š: {e}")
class LLMConnector:
    """å¼·åŒ–ã•ã‚ŒãŸOpenAI LLMæ¥ç¶šè¨­å®šã‚¯ãƒ©ã‚¹"""
    def __init__(self):
        self.is_online = check_internet_connection()
        self.selected_model = "gpt-3.5-turbo"
        self.openai_client = None
        self.security_manager = get_security_manager() if SECURITY_AVAILABLE else None
        self.ssl_context = setup_ssl_context()
        self._setup_session()
        
    def _setup_session(self):
        """HTTPã‚»ãƒƒã‚·ãƒ§ãƒ³ã®è¨­å®š"""
        self.session = requests.Session()
        
        # ãƒ˜ãƒƒãƒ€ãƒ¼ã®è¨­å®š
        self.session.headers.update({
            'User-Agent': 'RamanEye-Client/2.0',
            'Accept': 'application/json',
            'Content-Type': 'application/json',
            'X-Content-Type-Options': 'nosniff',
            'X-Frame-Options': 'DENY',
            'X-XSS-Protection': '1; mode=block'
        })
        
        # SSLè¨­å®š
        if self.ssl_context:
            adapter = requests.adapters.HTTPAdapter()
            self.session.mount('https://', adapter)
        
    def setup_llm_connection(self):
        """å¼·åŒ–ã•ã‚ŒãŸOpenAI APIæ¥ç¶šè¨­å®š"""
        # ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆæ¥ç¶šãƒã‚§ãƒƒã‚¯
        if not self.is_online:
            st.sidebar.error("âŒ ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆæ¥ç¶šãŒå¿…è¦ã§ã™")
            return False
        
        st.sidebar.success("ğŸŒ ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆæ¥ç¶š: æ­£å¸¸")
        
        # ãƒ¢ãƒ‡ãƒ«é¸æŠ
        model_options = [
            "gpt-3.5-turbo",
            "gpt-4",
            "gpt-4-turbo-preview"
        ]
        
        selected_model = st.sidebar.selectbox(
            "AI ãƒ¢ãƒ‡ãƒ«é¸æŠ",
            model_options,
            index=0,
            help="ä½¿ç”¨ã™ã‚‹AIãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠã—ã¦ãã ã•ã„"
        )
        
        try:
            # APIè¨­å®š
            openai.api_key = os.getenv("OPENAI_API_KEY", openai_api_key)
            
            # APIã‚­ãƒ¼ã®å¦¥å½“æ€§æ¤œè¨¼
            if not self._validate_api_key(openai.api_key):
                st.sidebar.error("ç„¡åŠ¹ãªAPIã‚­ãƒ¼ã§ã™")
                return False
            
            self.selected_model = selected_model
            self.openai_client = "openai"
            
            # ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ­ã‚°è¨˜éŒ²
            if self.security_manager:
                current_user = st.session_state.get('current_user', {})
                user_id = current_user.get('username', 'unknown')
                
                self.security_manager.audit_logger.log_security_event(
                    event_type="API_CONNECTION_SETUP",
                    user_id=user_id,
                    details={
                        'model': selected_model,
                        'ssl_enabled': self.ssl_context is not None,
                        'timestamp': datetime.now().isoformat()
                    },
                    severity="INFO"
                )
            
            st.sidebar.success(f"âœ… AI APIæ¥ç¶šè¨­å®šå®Œäº† ({selected_model})")
            return True
            
        except Exception as e:
            st.sidebar.error(f"APIè¨­å®šã‚¨ãƒ©ãƒ¼: {e}")
            
            # ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ­ã‚°è¨˜éŒ²
            if self.security_manager:
                current_user = st.session_state.get('current_user', {})
                user_id = current_user.get('username', 'unknown')
                
                self.security_manager.audit_logger.log_security_event(
                    event_type="API_CONNECTION_ERROR",
                    user_id=user_id,
                    details={'error': str(e)},
                    severity="ERROR"
                )
            
            return False
    
    def _validate_api_key(self, api_key: str) -> bool:
        """APIã‚­ãƒ¼ã®å¦¥å½“æ€§ã‚’æ¤œè¨¼"""
        if not api_key or len(api_key) < 20:
            return False
        
        # APIã‚­ãƒ¼ã®å½¢å¼ãƒã‚§ãƒƒã‚¯ï¼ˆOpenAIå½¢å¼ï¼‰
        if not api_key.startswith('sk-'):
            return False
        
        return True
    
    def generate_analysis(self, prompt, temperature=0.3, max_tokens=1024, stream_display=True):
        """å¼·åŒ–ã•ã‚ŒãŸOpenAI APIè§£æå®Ÿè¡Œ"""
        if not self.selected_model:
            raise SecurityException("AI ãƒ¢ãƒ‡ãƒ«ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¤ãƒ³ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³å¯¾ç­–
        sanitized_prompt = self._sanitize_prompt(prompt)
        
        system_message = "ã‚ãªãŸã¯ãƒ©ãƒãƒ³ã‚¹ãƒšã‚¯ãƒˆãƒ­ã‚¹ã‚³ãƒ”ãƒ¼ã®å°‚é–€å®¶ã§ã™ã€‚ãƒ”ãƒ¼ã‚¯ä½ç½®ã¨è«–æ–‡ã€ã¾ãŸã¯ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆä¸Šã®æƒ…å ±ã‚’æ¯”è¼ƒã—ã¦ã€ã“ã®ã‚µãƒ³ãƒ—ãƒ«ãŒä½•ã®è©¦æ–™ãªã®ã‹å½“ã¦ã¦ãã ã•ã„ã€‚ã™ã¹ã¦æ—¥æœ¬èªã§ç­”ãˆã¦ãã ã•ã„ã€‚"
        
        try:
            # ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ­ã‚°è¨˜éŒ²ï¼ˆãƒªã‚¯ã‚¨ã‚¹ãƒˆé–‹å§‹ï¼‰
            if self.security_manager:
                current_user = st.session_state.get('current_user', {})
                user_id = current_user.get('username', 'unknown')
                
                self.security_manager.audit_logger.log_security_event(
                    event_type="AI_ANALYSIS_REQUEST",
                    user_id=user_id,
                    details={
                        'model': self.selected_model,
                        'prompt_length': len(sanitized_prompt),
                        'temperature': temperature,
                        'max_tokens': max_tokens,
                        'timestamp': datetime.now().isoformat()
                    },
                    severity="INFO"
                )
            
            # HTTPSé€šä¿¡ã§APIå‘¼ã³å‡ºã—
            response = openai.ChatCompletion.create(
                model=self.selected_model,
                messages=[
                    {"role": "system", "content": system_message},
                    {"role": "user", "content": sanitized_prompt + "\n\nã™ã¹ã¦æ—¥æœ¬èªã§è©³ã—ãèª¬æ˜ã—ã¦ãã ã•ã„ã€‚"}
                ],
                temperature=temperature,
                max_tokens=max_tokens,
                stream=True,
                # è¨­å®š
                request_timeout=60,  # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆè¨­å®š
                api_version=None  # æœ€æ–°ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’å¼·åˆ¶
            )
            
            full_response = ""
            if stream_display:
                stream_area = st.empty()
            
            for chunk in response:
                if "choices" in chunk and len(chunk["choices"]) > 0:
                    delta = chunk["choices"][0]["delta"]
                    if "content" in delta:
                        content = self._sanitize_response_content(delta["content"])
                        full_response += content
                        if stream_display:
                            stream_area.markdown(full_response)
            
            # ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ­ã‚°è¨˜éŒ²ï¼ˆå¿œç­”å®Œäº†ï¼‰
            if self.security_manager:
                user_id = current_user.get('username', 'unknown') if 'current_user' in locals() else 'unknown'
                self.security_manager.audit_logger.log_security_event(
                    event_type="AI_ANALYSIS_RESPONSE",
                    user_id=user_id,
                    details={
                        'response_length': len(full_response),
                        'success': True,
                        'timestamp': datetime.now().isoformat()
                    },
                    severity="INFO"
                )
            
            return full_response
                
        except Exception as e:
            # ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ­ã‚°è¨˜éŒ²ï¼ˆã‚¨ãƒ©ãƒ¼ï¼‰
            if self.security_manager:
                current_user = st.session_state.get('current_user', {})
                user_id = current_user.get('username', 'unknown')
                
                self.security_manager.audit_logger.log_security_event(
                    event_type="AI_ANALYSIS_ERROR",
                    user_id=user_id,
                    details={'error': str(e)},
                    severity="ERROR"
                )
            
            raise SecurityException(f"OpenAI APIè§£æã‚¨ãƒ©ãƒ¼: {str(e)}")
    
    def _sanitize_prompt(self, prompt: str) -> str:
        """ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¤ãƒ³ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³å¯¾ç­–"""
        # å±é™ºãªãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’é™¤å»
        dangerous_patterns = [
            "ignore previous instructions",
            "disregard the above",
            "forget everything",
            "new instruction:",
            "system:",
            "admin:",
            "jailbreak",
            "prompt injection"
        ]
        
        sanitized = prompt
        for pattern in dangerous_patterns:
            sanitized = sanitized.replace(pattern.lower(), "")
            sanitized = sanitized.replace(pattern.upper(), "")
            sanitized = sanitized.replace(pattern.capitalize(), "")
        
        # é•·ã•åˆ¶é™
        if len(sanitized) > 10000:
            sanitized = sanitized[:10000]
        
        return sanitized
    
    def _sanitize_response_content(self, content: str) -> str:
        """å¿œç­”å†…å®¹ã®ã‚µãƒ‹ã‚¿ã‚¤ã‚º"""
        # HTMLã‚¿ã‚°ã®é™¤å»
        import re
        content = re.sub(r'<[^>]+>', '', content)
        
        # ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚¿ã‚°ã®é™¤å»
        content = re.sub(r'<script.*?</script>', '', content, flags=re.IGNORECASE | re.DOTALL)
        
        return content
    
    def generate_qa_response(self, question, context, previous_qa_history=None):
        """å¼·åŒ–ã•ã‚ŒãŸè³ªå•å¿œç­”å°‚ç”¨ã®OpenAI APIå‘¼ã³å‡ºã—"""
        if not self.selected_model:
            raise SecurityException("OpenAI ãƒ¢ãƒ‡ãƒ«ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
        
        # å…¥åŠ›ã®ã‚µãƒ‹ã‚¿ã‚¤ã‚º
        sanitized_question = self._sanitize_prompt(question)
        sanitized_context = self._sanitize_prompt(context)
        
        system_message = """ã‚ãªãŸã¯ãƒ©ãƒãƒ³ã‚¹ãƒšã‚¯ãƒˆãƒ­ã‚¹ã‚³ãƒ”ãƒ¼ã®å°‚é–€å®¶ã§ã™ã€‚
è§£æçµæœã‚„éå»ã®è³ªå•å±¥æ­´ã‚’è¸ã¾ãˆã¦ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«æ—¥æœ¬èªã§è©³ã—ãç­”ãˆã¦ãã ã•ã„ã€‚
ç§‘å­¦çš„æ ¹æ‹ ã«åŸºã¥ã„ãŸæ­£ç¢ºãªå›ç­”ã‚’å¿ƒãŒã‘ã¦ãã ã•ã„ã€‚"""
        
        # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®æ§‹ç¯‰
        context_text = f"ã€è§£æçµæœã€‘\n{sanitized_context}\n\n"
        
        if previous_qa_history:
            context_text += "ã€éå»ã®è³ªå•å±¥æ­´ã€‘\n"
            for i, qa in enumerate(previous_qa_history, 1):
                sanitized_prev_question = self._sanitize_prompt(qa['question'])
                sanitized_prev_answer = self._sanitize_prompt(qa['answer'])
                context_text += f"è³ªå•{i}: {sanitized_prev_question}\nå›ç­”{i}: {sanitized_prev_answer}\n\n"
        
        context_text += f"ã€æ–°ã—ã„è³ªå•ã€‘\n{sanitized_question}"
        
        try:
            # ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ­ã‚°è¨˜éŒ²
            if self.security_manager:
                current_user = st.session_state.get('current_user', {})
                user_id = current_user.get('username', 'unknown')
                
                self.security_manager.audit_logger.log_security_event(
                    event_type="QA_REQUEST",
                    user_id=user_id,
                    details={
                        'question_length': len(sanitized_question),
                        'context_length': len(sanitized_context),
                        'timestamp': datetime.now().isoformat()
                    },
                    severity="INFO"
                )
            
            response = openai.ChatCompletion.create(
                model=self.selected_model,
                messages=[
                    {"role": "system", "content": system_message},
                    {"role": "user", "content": context_text}
                ],
                temperature=0.3,
                max_tokens=1024,
                stream=True,
                request_timeout=60
            )
            
            full_response = ""
            stream_area = st.empty()
            
            for chunk in response:
                if "choices" in chunk and len(chunk["choices"]) > 0:
                    delta = chunk["choices"][0]["delta"]
                    if "content" in delta:
                        content = self._sanitize_response_content(delta["content"])
                        full_response += content
                        stream_area.markdown(full_response)
            
            return full_response
                
        except Exception as e:
            raise SecurityException(f"è³ªå•å¿œç­”ã‚¨ãƒ©ãƒ¼: {str(e)}")

class RamanRAGSystem:
    """å¼·åŒ–ã•ã‚ŒãŸRAGæ©Ÿèƒ½ã®ã‚¯ãƒ©ã‚¹"""
    def __init__(
        self,
        embedding_model_name: str = 'all-MiniLM-L6-v2',
        use_openai_embeddings: bool = True,
        openai_embedding_model: str = "text-embedding-ada-002"
    ):
        self.use_openai = use_openai_embeddings and check_internet_connection()
        self.openai_embedding_model = openai_embedding_model
        self.security_manager = get_security_manager() if SECURITY_AVAILABLE else None
        
        if self.use_openai:
            self.embedding_model = None
        else:
            self.embedding_model = SentenceTransformer(embedding_model_name)
        
        self.vector_db = None
        self.documents: List[str] = []
        self.document_metadata: List[Dict] = []
        self.embedding_dim: int = 0
        self.db_info: Dict = {}
    
    def build_vector_database(self, folder_path: str):
        """å¼·åŒ–ã•ã‚ŒãŸãƒ™ã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ§‹ç¯‰"""
        if not PDF_AVAILABLE:
            st.error("PDFå‡¦ç†ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")
            return
            
        if not os.path.exists(folder_path):
            st.error(f"æŒ‡å®šã•ã‚ŒãŸãƒ•ã‚©ãƒ«ãƒ€ãŒå­˜åœ¨ã—ã¾ã›ã‚“: {folder_path}")
            return

        # ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒã‚§ãƒƒã‚¯
        current_user = st.session_state.get('current_user', {})
        user_id = current_user.get('username', 'unknown')
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§å–å¾—ï¼ˆã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£è€ƒæ…®ï¼‰
        file_patterns = ['*.pdf', '*.docx', '*.txt']
        files = []
        for pat in file_patterns:
            potential_files = glob.glob(os.path.join(folder_path, pat))
            for file_path in potential_files:
                # ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ã‚¯ã‚»ã‚¹ã®ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒã‚§ãƒƒã‚¯
                if self.security_manager:
                    access_result = self.security_manager.secure_file_access(
                        file_path, user_id, 'read'
                    )
                    if access_result['status'] == 'success':
                        files.append(file_path)
                    else:
                        st.warning(f"ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ã‚¯ã‚»ã‚¹æ‹’å¦: {file_path}")
                else:
                    files.append(file_path)
        
        if not files:
            st.warning("ã‚¢ã‚¯ã‚»ã‚¹å¯èƒ½ãªãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
            return

        # ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºã¨ãƒãƒ£ãƒ³ã‚¯åŒ–ï¼ˆã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ä»˜ãï¼‰
        all_chunks, all_metadata = [], []
        st.info(f"{len(files)} ä»¶ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å®‰å…¨ã«å‡¦ç†ä¸­â€¦")
        pbar = st.progress(0)
        
        for idx, fp in enumerate(files):
            try:
                # ãƒ•ã‚¡ã‚¤ãƒ«å®Œå…¨æ€§ãƒã‚§ãƒƒã‚¯
                if self.security_manager:
                    integrity_result = self.security_manager.integrity_manager.verify_file_integrity(Path(fp))
                    if integrity_result['status'] == 'corrupted':
                        st.error(f"ãƒ•ã‚¡ã‚¤ãƒ«å®Œå…¨æ€§ã‚¨ãƒ©ãƒ¼: {fp}")
                        continue
                
                text = self._extract_text(fp)
                chunks = self.chunk_text(text)
                
                for c in chunks:
                    all_chunks.append(c)
                    all_metadata.append({
                        'filename': os.path.basename(fp),
                        'filepath': fp,
                        'preview': c[:100] + "â€¦" if len(c) > 100 else c,
                        'processed_by': user_id,
                        'processed_at': datetime.now().isoformat()
                    })
                    
            except Exception as e:
                st.error(f"ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ã‚¨ãƒ©ãƒ¼ {fp}: {e}")
                if self.security_manager:
                    self.security_manager.audit_logger.log_security_event(
                        event_type="FILE_PROCESSING_ERROR",
                        user_id=user_id,
                        details={'file_path': fp, 'error': str(e)},
                        severity="ERROR"
                    )
                continue
                
            pbar.progress((idx + 1) / len(files))

        if not all_chunks:
            st.error("æŠ½å‡ºã§ãã‚‹ãƒ†ã‚­ã‚¹ãƒˆãƒãƒ£ãƒ³ã‚¯ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
            return

        # åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã®ç”Ÿæˆ
        st.info("åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã‚’ç”Ÿæˆä¸­â€¦")
        try:
            if self.use_openai:
                embeddings = self._create_openai_embeddings(all_chunks)
            else:
                embeddings = self.embedding_model.encode(all_chunks, show_progress_bar=True)

            # FAISSã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰
            self.embedding_dim = embeddings.shape[1]
            index = faiss.IndexFlatIP(self.embedding_dim)
            faiss.normalize_L2(embeddings)
            index.add(embeddings)

            # çŠ¶æ…‹ä¿å­˜
            self.vector_db = index
            self.documents = all_chunks
            self.document_metadata = all_metadata
            self.db_info = {
                'created_at': datetime.now().isoformat(),
                'created_by': user_id,
                'n_docs': len(files),
                'n_chunks': len(all_chunks),
                'source_files': [os.path.basename(f) for f in files],
                'embedding_model': (
                    self.openai_embedding_model if self.use_openai 
                    else self.embedding_model.__class__.__name__
                ),
                'security_enabled': SECURITY_AVAILABLE
            }
            
            # ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ­ã‚°è¨˜éŒ²
            if self.security_manager:
                self.security_manager.audit_logger.log_security_event(
                    event_type="VECTOR_DB_CREATED",
                    user_id=user_id,
                    details={
                        'n_chunks': len(all_chunks),
                        'n_files': len(files),
                        'embedding_model': self.db_info['embedding_model']
                    },
                    severity="INFO"
                )
            
            st.success(f"ãƒ™ã‚¯ãƒˆãƒ«DBæ§‹ç¯‰å®Œäº†: {len(all_chunks)} ãƒãƒ£ãƒ³ã‚¯")
            
        except Exception as e:
            st.error(f"ãƒ™ã‚¯ãƒˆãƒ«DBæ§‹ç¯‰ã‚¨ãƒ©ãƒ¼: {e}")
            if self.security_manager:
                self.security_manager.audit_logger.log_security_event(
                    event_type="VECTOR_DB_ERROR",
                    user_id=user_id,
                    details={'error': str(e)},
                    severity="ERROR"
                )
    
    def _create_openai_embeddings(self, texts: List[str], batch_size: int = 200) -> np.ndarray:
        """OpenAIåŸ‹ã‚è¾¼ã¿APIã®ä½¿ç”¨"""
        all_embs = []
        
        # ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ­ã‚°è¨˜éŒ²
        if self.security_manager:
            current_user = st.session_state.get('current_user', {})
            user_id = current_user.get('username', 'unknown')
            
            self.security_manager.audit_logger.log_security_event(
                event_type="OPENAI_EMBEDDING_REQUEST",
                user_id=user_id,
                details={
                    'num_texts': len(texts),
                    'batch_size': batch_size,
                    'model': self.openai_embedding_model
                },
                severity="INFO"
            )
        
        try:
            for i in range(0, len(texts), batch_size):
                chunk = texts[i:i+batch_size]
                
                # ãƒ†ã‚­ã‚¹ãƒˆã®å‰å‡¦ç†ãƒ»ã‚µãƒ‹ã‚¿ã‚¤ã‚º
                sanitized_chunk = []
                for text in chunk:
                    # é•·ã™ãã‚‹ãƒ†ã‚­ã‚¹ãƒˆã®ãƒˆãƒ©ãƒ³ã‚±ãƒ¼ãƒˆ
                    if len(text) > 8000:  # OpenAIåˆ¶é™ã«åˆã‚ã›ã¦èª¿æ•´
                        text = text[:8000]
                    sanitized_chunk.append(text)
                
                # HTTPSé€šä¿¡ã§APIå‘¼ã³å‡ºã—
                resp = openai.Embedding.create(
                    model=self.openai_embedding_model,
                    input=sanitized_chunk,
                    timeout=60
                )
                
                embs = [d['embedding'] for d in resp['data']]
                all_embs.extend(embs)
                
                # é€²æ—è¡¨ç¤º
                if len(texts) > batch_size:
                    progress = min(i + batch_size, len(texts)) / len(texts)
                    st.progress(progress)
                    
        except Exception as e:
            if self.security_manager:
                user_id = current_user.get('username', 'unknown') if 'current_user' in locals() else 'unknown'
                self.security_manager.audit_logger.log_security_event(
                    event_type="OPENAI_EMBEDDING_ERROR",
                    user_id=user_id,
                    details={'error': str(e)},
                    severity="ERROR"
                )
            raise SecurityException(f"åŸ‹ã‚è¾¼ã¿ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
        
        return np.array(all_embs, dtype=np.float32)
    
    def _extract_text(self, file_path: str) -> str:
        """ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ã®ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º"""
        ext = os.path.splitext(file_path)[1].lower()
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºãƒã‚§ãƒƒã‚¯
        file_size = os.path.getsize(file_path)
        MAX_FILE_SIZE = 100 * 1024 * 1024  # 100MBåˆ¶é™
        if file_size > MAX_FILE_SIZE:
            raise SecurityException(f"ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºãŒåˆ¶é™ã‚’è¶…ãˆã¦ã„ã¾ã™: {file_path}")
        
        try:
            if ext == '.pdf':
                reader = PyPDF2.PdfReader(file_path)
                text_parts = []
                for page_num, page in enumerate(reader.pages):
                    try:
                        page_text = page.extract_text() or ""
                        text_parts.append(page_text)
                    except Exception as e:
                        st.warning(f"PDF ãƒšãƒ¼ã‚¸ {page_num} èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
                return "\n".join(text_parts)
                
            elif ext == '.docx':
                doc = docx.Document(file_path)
                return "\n".join(p.text for p in doc.paragraphs)
                
            elif ext == '.txt':
                with open(file_path, encoding='utf-8', errors='ignore') as f:
                    content = f.read()
                # ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚µã‚¤ã‚ºåˆ¶é™
                if len(content) > 1000000:  # 1MBåˆ¶é™
                    content = content[:1000000]
                return content
                
        except Exception as e:
            st.error(f"ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºã‚¨ãƒ©ãƒ¼ {file_path}: {e}")
            return ""
    
    def chunk_text(self, text: str, chunk_size: int = 500, overlap: int = 50) -> List[str]:
        """ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²"""
        if not text or not text.strip():
            return []
        
        # ãƒ†ã‚­ã‚¹ãƒˆã®å‰å‡¦ç†
        text = text.strip()
        
        # å±é™ºãªã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        if self._contains_malicious_content(text):
            st.warning("æ½œåœ¨çš„ã«å±é™ºãªã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸã€‚å‡¦ç†ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
            return []
        
        words = text.split()
        chunks = []
        
        for i in range(0, len(words), chunk_size - overlap):
            chunk = ' '.join(words[i:i + chunk_size])
            if chunk.strip() and len(chunk) > 10:  # çŸ­ã™ãã‚‹ãƒãƒ£ãƒ³ã‚¯ã‚’é™¤å¤–
                chunks.append(chunk)
                
        return chunks
    
    def _contains_malicious_content(self, text: str) -> bool:
        """æ‚ªæ„ã®ã‚ã‚‹ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®æ¤œå‡º"""
        # åŸºæœ¬çš„ãªãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°
        malicious_patterns = [
            r'<script.*?>',
            r'javascript:',
            r'vbscript:',
            r'onload=',
            r'onerror=',
            r'eval\(',
            r'document\.cookie',
            r'window\.location'
        ]
        
        import re
        text_lower = text.lower()
        
        for pattern in malicious_patterns:
            if re.search(pattern, text_lower, re.IGNORECASE):
                return True
        
        return False
    
    def search_relevant_documents(self, query: str, top_k: int = 5) -> List[Dict]:
        """é–¢é€£æ–‡æ›¸æ¤œç´¢"""
        if self.vector_db is None:
            return []
        
        try:
            # ã‚¯ã‚¨ãƒªã®ã‚µãƒ‹ã‚¿ã‚¤ã‚º
            sanitized_query = query.strip()
            if len(sanitized_query) > 1000:
                sanitized_query = sanitized_query[:1000]
            
            # ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ­ã‚°è¨˜éŒ²
            if self.security_manager:
                current_user = st.session_state.get('current_user', {})
                user_id = current_user.get('username', 'unknown')
                
                self.security_manager.audit_logger.log_security_event(
                    event_type="DOCUMENT_SEARCH",
                    user_id=user_id,
                    details={
                        'query_length': len(sanitized_query),
                        'top_k': top_k
                    },
                    severity="INFO"
                )
    
            # DBä½œæˆæ™‚ã®ãƒ¢ãƒ‡ãƒ«æƒ…å ±ã‚’ç¢ºèª
            model_used = self.db_info.get("embedding_model", "")
            if model_used == "text-embedding-ada-002":
                query_emb = self._create_openai_embeddings([sanitized_query])
            else:
                query_emb = self.embedding_model.encode([sanitized_query], show_progress_bar=False)
    
            query_emb = np.array(query_emb, dtype=np.float32)
            faiss.normalize_L2(query_emb)
            
            # é¡ä¼¼æ–‡æ›¸ã‚’æ¤œç´¢
            scores, indices = self.vector_db.search(query_emb, top_k)
            
            results = []
            for score, idx in zip(scores[0], indices[0]):
                if idx < len(self.documents):
                    results.append({
                        'text': self.documents[idx],
                        'metadata': self.document_metadata[idx],
                        'similarity_score': float(score)
                    })
            
            return results
            
        except Exception as e:
            st.error(f"æ–‡æ›¸æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
            return []
    
    def save_database(self, save_path: str, db_name: str = "raman_rag_database"):
        """æ§‹ç¯‰ã—ãŸãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’ä¿å­˜"""
        if self.vector_db is None:
            st.error("ä¿å­˜ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãŒå­˜åœ¨ã—ã¾ã›ã‚“ã€‚")
            return False
        
        try:
            save_folder = Path(save_path)
            save_folder.mkdir(parents=True, exist_ok=True)
            
            # FAISSã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä¿å­˜
            faiss_path = save_folder / f"{db_name}_faiss.index"
            faiss.write_index(self.vector_db, str(faiss_path))
            
            # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã¨ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
            documents_path = save_folder / f"{db_name}_documents.pkl"
            with open(documents_path, 'wb') as f:
                pickle.dump({
                    'documents': self.documents,
                    'document_metadata': self.document_metadata,
                    'embedding_dim': self.embedding_dim
                }, f)
            
            # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æƒ…å ±ã‚’ä¿å­˜
            info_path = save_folder / f"{db_name}_info.json"
            with open(info_path, 'w', encoding='utf-8') as f:
                json.dump(self.db_info, f, ensure_ascii=False, indent=2)
            
            st.success(f"âœ… ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {save_folder}")
            st.info(f"ğŸ“ ä¿å­˜ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«:\n"
                   f"- {db_name}_faiss.index (FAISSãƒ™ã‚¯ãƒˆãƒ«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹)\n"
                   f"- {db_name}_documents.pkl (ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿)\n"
                   f"- {db_name}_info.json (ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æƒ…å ±)")
            
            return True
            
        except Exception as e:
            st.error(f"ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def get_database_info(self) -> Dict:
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®æƒ…å ±ã‚’å–å¾—"""
        if self.vector_db is None:
            return {"status": "ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãŒæ§‹ç¯‰ã•ã‚Œã¦ã„ã¾ã›ã‚“"}
        
        info = self.db_info.copy()
        info["status"] = "æ§‹ç¯‰æ¸ˆã¿"
        info["current_chunks"] = len(self.documents)
        return info

class RamanSpectrumAnalyzer:
    """ãƒ©ãƒãƒ³ã‚¹ãƒšã‚¯ãƒˆãƒ«è§£æã‚¯ãƒ©ã‚¹"""
    def generate_analysis_prompt(self, peak_data: List[Dict], relevant_docs: List[Dict], user_hint: Optional[str] = None) -> str:
        """ãƒ©ãƒãƒ³ã‚¹ãƒšã‚¯ãƒˆãƒ«è§£æã®ãŸã‚ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç”Ÿæˆ"""
        
        def format_peaks(peaks: List[Dict]) -> str:
            header = "ã€æ¤œå‡ºãƒ”ãƒ¼ã‚¯ä¸€è¦§ã€‘"
            lines = [
                f"{i+1}. æ³¢æ•°: {p.get('wavenumber', 0):.1f} cmâ»Â¹, "
                f"å¼·åº¦: {p.get('intensity', 0):.3f}, "
                f"å“ç«‹åº¦: {p.get('prominence', 0):.3f}, "
                f"ç¨®é¡: {'è‡ªå‹•æ¤œå‡º' if p.get('type') == 'auto' else 'æ‰‹å‹•è¿½åŠ '}"
                for i, p in enumerate(peaks)
            ]
            return "\n".join([header] + lines)

        def format_reference_excerpts(docs: List[Dict]) -> str:
            header = "ã€å¼•ç”¨æ–‡çŒ®ã®æŠœç²‹ã¨è¦ç´„ã€‘"
            lines = []
            for i, doc in enumerate(docs, 1):
                title = doc.get("metadata", {}).get("filename", f"æ–‡çŒ®{i}")
                page = doc.get("metadata", {}).get("page")
                summary = doc.get("page_content", "").strip()
                lines.append(f"\n--- å¼•ç”¨{i} ---")
                lines.append(f"å‡ºå…¸ãƒ•ã‚¡ã‚¤ãƒ«: {title}")
                if page is not None:
                    lines.append(f"ãƒšãƒ¼ã‚¸ç•ªå·: {page}")
                lines.append(f"æŠœç²‹å†…å®¹:\n{summary}")
            return "\n".join([header] + lines)

        def format_doc_summaries(docs: List[Dict], preview_length: int = 300) -> str:
            header = "ã€æ–‡çŒ®ã®æ¦‚è¦ï¼ˆé¡ä¼¼åº¦ä»˜ãï¼‰ã€‘"
            lines = []
            for i, doc in enumerate(docs, 1):
                filename = doc.get("metadata", {}).get("filename", f"æ–‡çŒ®{i}")
                similarity = doc.get("similarity_score", 0.0)
                text = doc.get("text") or doc.get("page_content") or ""
                lines.append(
                    f"æ–‡çŒ®{i} (é¡ä¼¼åº¦: {similarity:.3f})\n"
                    f"ãƒ•ã‚¡ã‚¤ãƒ«å: {filename}\n"
                    f"å†’é ­æŠœç²‹: {text.strip()[:preview_length]}...\n"
                )
            return "\n".join([header] + lines)

        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæœ¬æ–‡ã®æ§‹ç¯‰
        sections = [
            "ä»¥ä¸‹ã¯ã€ãƒ©ãƒãƒ³ã‚¹ãƒšã‚¯ãƒˆãƒ«ã§æ¤œå‡ºã•ã‚ŒãŸãƒ”ãƒ¼ã‚¯æƒ…å ±ã§ã™ã€‚",
            "ã“ã‚Œã‚‰ã®ãƒ”ãƒ¼ã‚¯ã«åŸºã¥ãã€è©¦æ–™ã®æˆåˆ†ã‚„ç‰¹å¾´ã«ã¤ã„ã¦æ¨å®šã—ã¦ãã ã•ã„ã€‚",
            "ãªãŠã€æ–‡çŒ®ã¨ã®æ¯”è¼ƒã«ãŠã„ã¦ã¯ãƒ”ãƒ¼ã‚¯ä½ç½®ãŒÂ±5cmâ»Â¹ç¨‹åº¦ãšã‚Œã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚",
            "ãã®ãŸã‚ã€Â±5cmâ»Â¹ä»¥å†…ã®å·®ã§ã‚ã‚Œã°ä¸€è‡´ã¨ã¿ãªã—ã¦è§£æã‚’è¡Œã£ã¦ãã ã•ã„ã€‚\n"
        ]

        if user_hint:
            sections.append(f"ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã‚ˆã‚‹è£œè¶³æƒ…å ±ã€‘\n{user_hint}\n")

        if peak_data:
            sections.append(format_peaks(peak_data))
        if relevant_docs:
            sections.append(format_reference_excerpts(relevant_docs))
            sections.append(format_doc_summaries(relevant_docs))

        sections.append(
            "ã“ã‚Œã‚‰ã‚’å‚è€ƒã«ã€è©¦æ–™ã«å«ã¾ã‚Œã‚‹å¯èƒ½æ€§ã®ã‚ã‚‹åŒ–åˆç‰©ã‚„ç‰©è³ªæ§‹é€ ã€ç‰¹å¾´ã«ã¤ã„ã¦è©³ã—ãèª¬æ˜ã—ã¦ãã ã•ã„ã€‚\n"
            "å‡ºåŠ›ã¯æ—¥æœ¬èªã§ãŠé¡˜ã„ã—ã¾ã™ã€‚\n"
            "## è§£æã®è¦³ç‚¹:\n"
            "1. å„ãƒ”ãƒ¼ã‚¯ã®åŒ–å­¦çš„å¸°å±ã¨ãã®æ ¹æ‹ \n"
            "2. è©¦æ–™ã®å¯èƒ½ãªçµ„æˆã‚„æ§‹é€ \n"
            "3. æ–‡çŒ®æƒ…å ±ã¨ã®æ¯”è¼ƒãƒ»å¯¾ç…§\n\n"
            "è©³ç´°ã§ç§‘å­¦çš„æ ¹æ‹ ã«åŸºã¥ã„ãŸè€ƒå¯Ÿã‚’æ—¥æœ¬èªã§æä¾›ã—ã¦ãã ã•ã„ã€‚"
        )

        return "\n".join(sections)

# === æ–°æ©Ÿèƒ½: PDFãƒ¬ãƒãƒ¼ãƒˆã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ ===
class RamanPDFReportGenerator:
    """ãƒ©ãƒãƒ³ã‚¹ãƒšã‚¯ãƒˆãƒ«è§£æPDFãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.temp_dir = "./temp_pdf_assets"
        os.makedirs(self.temp_dir, exist_ok=True)
        
        # æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆã®è¨­å®šã‚’è©¦è¡Œ
        self.setup_japanese_font()
        
        # ãƒ¬ãƒãƒ¼ãƒˆã‚¹ã‚¿ã‚¤ãƒ«ã®è¨­å®š
        self.setup_styles()
        
    def _sanitize_text_for_pdf(self, text: str) -> str:
        """PDFã«å®‰å…¨ãªãƒ†ã‚­ã‚¹ãƒˆã«å¤‰æ›"""
        if text is None:
            return ""
        
        # HTMLã‚¨ã‚¹ã‚±ãƒ¼ãƒ—å‡¦ç†
        text = str(text)
        text = text.replace('&', '&amp;')
        text = text.replace('<', '&lt;')
        text = text.replace('>', '&gt;')
        text = text.replace('"', '&quot;')
        text = text.replace("'", '&#39;')
        
        # æ”¹è¡Œã®æ­£è¦åŒ–
        text = text.replace('\r\n', '\n').replace('\r', '\n')
        
        # åˆ¶å¾¡æ–‡å­—ã‚’é™¤å»
        import re
        text = re.sub(r'[\x00-\x08\x0B\x0C\x0E-\x1F\x7F]', '', text)
        
        return text
    def setup_japanese_font(self):
        """æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆã®è¨­å®šï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æˆ¦ç•¥å¼·åŒ–ï¼‰"""
        self.japanese_font_available = False
        pdfmetrics.registerFont(UnicodeCIDFont('HeiseiKakuGo-W5'))
        self.japanese_font_name = 'HeiseiKakuGo-W5'  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚’å¿…ãšå®šç¾©

        try:
            # ã‚·ã‚¹ãƒ†ãƒ ã«ã‚ã‚‹æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆã‚’æ¢ã™
            font_paths = [
            # Windows - æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆ
            "C:/Windows/Fonts/yumin.ttf",
            
            """
            "C:/Windows/Fonts/yumin.ttf",
            "C:/Windows/Fonts/msgothic.ttc",
            "C:/Windows/Fonts/meiryo.ttc", 
            "C:/Windows/Fonts/meiryob.ttc",
            "C:/Windows/Fonts/YuGothic.ttc",
            "C:/Windows/Fonts/YuGothM.ttc",
            "C:/Windows/Fonts/NotoSansCJK-Regular.ttc",
            "C:/Windows/Fonts/NotoSansJP-Regular.otf",
            
            # macOS - æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆ
            "/System/Library/Fonts/ãƒ’ãƒ©ã‚®ãƒè§’ã‚´ã‚·ãƒƒã‚¯ W3.ttc",
            "/System/Library/Fonts/ãƒ’ãƒ©ã‚®ãƒè§’ã‚´ ProN W3.otf",
            "/Library/Fonts/NotoSansCJK.ttc",
            "/Library/Fonts/NotoSansJP-Regular.otf",
            
            # Linux - æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆå„ªå…ˆ
            "/usr/share/fonts/truetype/noto/NotoSansCJK-Regular.ttc",
            "/usr/share/fonts/opentype/noto/NotoSansCJK-Regular.ttc",
            "/usr/share/fonts/truetype/noto/NotoSansJP-Regular.ttf",
            "/usr/share/fonts/opentype/noto/NotoSansJP-Regular.otf",
            "/usr/share/fonts/truetype/vlgothic/VL-Gothic-Regular.ttf",
            "/usr/share/fonts/truetype/vlgothic/VL-PGothic-Regular.ttf",
            
            # æ±ç”¨ãƒ•ã‚©ãƒ³ãƒˆï¼ˆæœ€å¾Œã®æ‰‹æ®µï¼‰
            "/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf",
            "/usr/share/fonts/TTF/DejaVuSans.ttf",
            "/usr/share/fonts/truetype/liberation/LiberationSans-Regular.ttf"
            """
        ]
            
            for font_path in font_paths:
                if os.path.exists(font_path):
                    try:
                        pdfmetrics.registerFont(TTFont('JapaneseFont', font_path))
                        self.japanese_font_available = True
                        self.japanese_font_name = 'JapaneseFont'
                        st.info(f"ãƒ•ã‚©ãƒ³ãƒˆä½¿ç”¨: {os.path.basename(font_path)}")
                        break
                    except Exception as e:
                        continue
            """
            if not self.japanese_font_available:
                # ã‚ˆã‚Šæ±ç”¨çš„ãªãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                try:
                    # ReportLabã®æ¨™æº–ãƒ•ã‚©ãƒ³ãƒˆã‚’ä½¿ç”¨
                    self.japanese_font_name = 'Times-Roman'
                    st.info("æ¨™æº–ãƒ•ã‚©ãƒ³ãƒˆï¼ˆTimes-Romanï¼‰ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚æ—¥æœ¬èªã¯æ–‡å­—åŒ–ã‘ã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚")
                except:
                    self.japanese_font_name = 'Helvetica'
                    st.info("åŸºæœ¬ãƒ•ã‚©ãƒ³ãƒˆï¼ˆHelveticaï¼‰ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚æ—¥æœ¬èªã¯æ–‡å­—åŒ–ã‘ã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚")
            """
        except Exception as e:
            self.japanese_font_name = 'Helvetica'
            st.info(f"ãƒ•ã‚©ãƒ³ãƒˆè¨­å®šè­¦å‘Š: {e}. åŸºæœ¬ãƒ•ã‚©ãƒ³ãƒˆã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
    
    def setup_styles(self):
        """PDFã‚¹ã‚¿ã‚¤ãƒ«ã®è¨­å®š"""
        self.styles = getSampleStyleSheet()
        
        # ã‚«ã‚¹ã‚¿ãƒ ã‚¹ã‚¿ã‚¤ãƒ«ã®è¿½åŠ 
        self.styles.add(ParagraphStyle(
            name='JapaneseTitle',
            parent=self.styles['Title'],
            fontName=self.japanese_font_name,
            fontSize=18,
            spaceAfter=20,
            alignment=TA_CENTER
        ))
        
        self.styles.add(ParagraphStyle(
            name='JapaneseHeading',
            parent=self.styles['Heading1'],
            fontName=self.japanese_font_name,
            fontSize=14,
            spaceAfter=12,
            textColor=colors.darkblue
        ))
        
        self.styles.add(ParagraphStyle(
            name='JapaneseNormal',
            parent=self.styles['Normal'],
            fontName=self.japanese_font_name,
            fontSize=10,
            spaceAfter=6,
            alignment=TA_JUSTIFY
        ))
        
    def plotly_to_image(self, fig, filename, width=800, height=600, format='png'):
        """Plotlyã‚°ãƒ©ãƒ•ã‚’PNGç”»åƒã«å¤‰æ›ï¼ˆæ”¹è‰¯ç‰ˆï¼‰"""
        try:
            # ç”»åƒã¨ã—ã¦ä¿å­˜
            img_path = os.path.join(self.temp_dir, f"{filename}.{format}")
            
            # Plotlyã®è¨­å®š
            fig.update_layout(
                width=width,
                height=height,
                font=dict(size=10),
                margin=dict(l=50, r=50, t=80, b=50)
            )
            
            # ç”»åƒä¿å­˜ï¼ˆè¤‡æ•°ã®æ–¹æ³•ã‚’è©¦è¡Œï¼‰
            success = False
            
            # æ–¹æ³•1: kaleidoä½¿ç”¨ã‚’è©¦è¡Œ
            try:
                pio.write_image(fig, img_path, format=format, width=width, height=height, scale=2)
                success = True
                st.info("Kaleidoã‚¨ãƒ³ã‚¸ãƒ³ã§ã‚°ãƒ©ãƒ•ã‚’ç”»åƒåŒ–ã—ã¾ã—ãŸ")
            except Exception as kaleido_error:
                st.warning(f"Kaleidoã‚¨ãƒ³ã‚¸ãƒ³ä½¿ç”¨å¤±æ•—: {str(kaleido_error)}")
            
        except Exception as e:
            st.error(f"ã‚°ãƒ©ãƒ•ç”»åƒå¤‰æ›ã‚¨ãƒ©ãƒ¼: {e}")
            # æœ€çµ‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            placeholder_path = os.path.join(self.temp_dir, f"{filename}_fallback.png")
            self._create_enhanced_placeholder_image(placeholder_path, width, height, f"Graph Error: {filename}")
            return placeholder_path

    def _create_enhanced_placeholder_image(self, path, width, height, text):
        """é«˜å“è³ªãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ç”»åƒã‚’ä½œæˆ"""
        try:
            from PIL import Image, ImageDraw, ImageFont
            
            # ç™½èƒŒæ™¯ã®ç”»åƒã‚’ä½œæˆ
            img = PILImage.new('RGB', (width, height), color='white')
            draw = ImageDraw.Draw(img)
            
            # ã‚°ãƒ©ãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³èƒŒæ™¯ã‚’ä½œæˆ
            for y in range(height):
                color_value = int(255 - (y / height) * 20)  # è–„ã„ã‚°ãƒ©ãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
                color = (color_value, color_value, color_value)
                draw.line([(0, y), (width, y)], fill=color)
            
            # ãƒ•ã‚©ãƒ³ãƒˆã‚’è¨­å®š
            try:
                # ã‚ˆã‚Šå¤§ããªãƒ•ã‚©ãƒ³ãƒˆã‚’è©¦è¡Œ
                font_large = ImageFont.load_default()
                font_small = ImageFont.load_default()
            except:
                font_large = None
                font_small = None
            
            # ã‚¿ã‚¤ãƒˆãƒ«ãƒ†ã‚­ã‚¹ãƒˆ
            title_text = "Raman Spectrum Analysis"
            if hasattr(draw, 'textbbox'):
                title_bbox = draw.textbbox((0, 0), title_text, font=font_large)
                title_width = title_bbox[2] - title_bbox[0]
                title_height = title_bbox[3] - title_bbox[1]
            else:
                title_width, title_height = 200, 20
            
            title_x = (width - title_width) // 2
            title_y = height // 4
            
            draw.text((title_x, title_y), title_text, fill='darkblue', font=font_large)
            
            # ã‚µãƒ–ã‚¿ã‚¤ãƒˆãƒ«
            subtitle = text
            if hasattr(draw, 'textbbox'):
                sub_bbox = draw.textbbox((0, 0), subtitle, font=font_small)
                sub_width = sub_bbox[2] - sub_bbox[0]
            else:
                sub_width = len(subtitle) * 8
            
            sub_x = (width - sub_width) // 2
            sub_y = title_y + title_height + 20
            
            draw.text((sub_x, sub_y), subtitle, fill='black', font=font_small)
            
            # ç°¡å˜ãªã‚°ãƒ©ãƒ•é¢¨ã®è£…é£¾ã‚’è¿½åŠ 
            # Xè»¸
            draw.line([(width//8, height*3//4), (width*7//8, height*3//4)], fill='black', width=2)
            # Yè»¸
            draw.line([(width//8, height//8), (width//8, height*3//4)], fill='black', width=2)
            
            # ã‚µãƒ³ãƒ—ãƒ«æ³¢å½¢
            points = []
            for i in range(width//8, width*7//8, 5):
                x = i
                y = height//2 + int(50 * np.sin((i - width//8) * 0.01)) + int(30 * np.sin((i - width//8) * 0.03))
                points.append((x, y))
            
            if len(points) > 1:
                draw.line(points, fill='blue', width=2)
            
            # ã„ãã¤ã‹ã®ãƒ”ãƒ¼ã‚¯ç‚¹ã‚’è¿½åŠ 
            peak_points = [(width//3, height//2 - 20), (width*2//3, height//2 - 40)]
            for px, py in peak_points:
                draw.ellipse([px-4, py-4, px+4, py+4], fill='red')
            
            # æ ç·šã‚’æç”»
            draw.rectangle([0, 0, width-1, height-1], outline='gray', width=2)
            
            # æ³¨æ„æ›¸ã
            note_text = "Note: Graph generated without Kaleido engine"
            note_y = height - 30
            if hasattr(draw, 'textbbox'):
                note_bbox = draw.textbbox((0, 0), note_text, font=font_small)
                note_width = note_bbox[2] - note_bbox[0]
            else:
                note_width = len(note_text) * 6
            
            note_x = (width - note_width) // 2
            draw.text((note_x, note_y), note_text, fill='gray', font=font_small)
            
            img.save(path)
            
        except Exception as e:
            # æœ€ã‚‚åŸºæœ¬çš„ãªãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼
            try:
                img = PILImage.new('RGB', (width, height), color='lightgray')
                draw = ImageDraw.Draw(img)
                
                simple_text = "Graph Placeholder"
                text_x = width // 2 - 50
                text_y = height // 2 - 10
                draw.text((text_x, text_y), simple_text, fill='black')
                
                img.save(path)
            except:
                st.warning(f"ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ç”»åƒä½œæˆæœ€çµ‚ã‚¨ãƒ©ãƒ¼: {e}")
    
    def _create_placeholder_image(self, path, width, height, text):
        """ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ç”»åƒã‚’ä½œæˆ"""
        try:
            from PIL import Image, ImageDraw, ImageFont
            
            # ç™½èƒŒæ™¯ã®ç”»åƒã‚’ä½œæˆ
            img = PILImage.new('RGB', (width, height), color='white')
            draw = ImageDraw.Draw(img)
            
            # ãƒ†ã‚­ã‚¹ãƒˆã‚’ä¸­å¤®ã«æç”»
            try:
                font = ImageFont.load_default()
            except:
                font = None
            
            # ãƒ†ã‚­ã‚¹ãƒˆã®ã‚µã‚¤ã‚ºã‚’è¨ˆç®—
            if hasattr(draw, 'textbbox'):
                bbox = draw.textbbox((0, 0), text, font=font)
                text_width = bbox[2] - bbox[0]
                text_height = bbox[3] - bbox[1]
            else:
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                text_width, text_height = draw.textsize(text, font=font) if hasattr(draw, 'textsize') else (100, 20)
            
            x = (width - text_width) // 2
            y = (height - text_height) // 2
            
            draw.text((x, y), text, fill='black', font=font)
            
            # æ ç·šã‚’æç”»
            draw.rectangle([0, 0, width-1, height-1], outline='gray')
            
            img.save(path)
            
        except Exception as e:
            st.warning(f"ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ç”»åƒä½œæˆã‚¨ãƒ©ãƒ¼: {e}")
    
    def generate_pdf_report(
        self, 
        file_key: str,
        peak_data: List[Dict],
        analysis_result: str,
        peak_summary_df: pd.DataFrame,
        plotly_figure: go.Figure = None,
        relevant_docs: List[Dict] = None,
        user_hint: str = None,
        qa_history: List[Dict] = None,
        database_info: Dict = None,  # ã“ã®è¡Œã‚’è¿½åŠ 
        database_files: List[str] = None,  # ã“ã®è¡Œã‚’è¿½åŠ 
        original_spectrum_data: Dict = None,  # ã“ã®è¡Œã‚’è¿½åŠ 
    ) -> bytes:

        """PDFãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ"""
        
        if not PDF_GENERATION_AVAILABLE:
            raise Exception("PDFç”Ÿæˆãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")
        
        # PDFãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ¡ãƒ¢ãƒªä¸Šã«ä½œæˆ
        import io
        pdf_buffer = io.BytesIO()
        
        try:
            # SimpleDocTemplateã§PDFä½œæˆ
            doc = SimpleDocTemplate(
                pdf_buffer,
                pagesize=A4,
                rightMargin=2*cm,
                leftMargin=2*cm,
                topMargin=2*cm,
                bottomMargin=2*cm
            )
            
            # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒªã‚¹ãƒˆã‚’ä½œæˆ
            story = []
            
            # 1. ã‚¿ã‚¤ãƒˆãƒ«ãƒšãƒ¼ã‚¸
            story.extend(self._create_title_page(file_key))
            
            # 2. å®Ÿè¡Œã‚µãƒãƒªãƒ¼
            story.extend(self._create_executive_summary(peak_data, analysis_result))
            
            # 3. ã‚°ãƒ©ãƒ•ã‚»ã‚¯ã‚·ãƒ§ãƒ³
            if original_spectrum_data:
                story.extend(self._create_graph_section_from_original_data(original_spectrum_data, file_key))
        
            # 4. ãƒ”ãƒ¼ã‚¯è©³ç´°ãƒ†ãƒ¼ãƒ–ãƒ«
            story.extend(self._create_peak_details_section(peak_summary_df, peak_data))
            
            # 5. AIè§£æçµæœ
            story.extend(self._create_ai_analysis_section(analysis_result))
            
            # 6. å‚è€ƒæ–‡çŒ®ï¼ˆåˆ©ç”¨å¯èƒ½ãªå ´åˆï¼‰
            if relevant_docs:
                story.extend(self._create_references_section(relevant_docs))
            
            # 7. è£œè¶³æƒ…å ±
            if user_hint:
                story.extend(self._create_additional_info_section(user_hint))
            
            # 8. Q&Aå±¥æ­´ï¼ˆåˆ©ç”¨å¯èƒ½ãªå ´åˆï¼‰
            if qa_history:
                story.extend(self._create_qa_section(qa_history))
            
            # 9. ä»˜éŒ²ãƒ»ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
            story.extend(self._create_appendix_section())
            
            # PDFã‚’æ§‹ç¯‰
            doc.build(story)
            
            # ãƒã‚¤ãƒˆé…åˆ—ã¨ã—ã¦è¿”ã™
            pdf_bytes = pdf_buffer.getvalue()
            pdf_buffer.close()
            
            return pdf_bytes
            
        except Exception as e:
            st.error(f"PDFãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            raise e
    
    def _create_graph_section_from_original_data(self, spectrum_data: Dict, file_key: str) -> List:
        """å…ƒã®ã‚¹ãƒšã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ç›´æ¥ã‚°ãƒ©ãƒ•ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’ä½œæˆ"""
        content = []
        
        heading_text = self._sanitize_text_for_pdf("ã‚¹ãƒšã‚¯ãƒˆãƒ«ãŠã‚ˆã³ãƒ”ãƒ¼ã‚¯æ¤œå‡ºçµæœ" if self.japanese_font_available else "Spectrum and Peak Detection Results")
        content.append(Paragraph(heading_text, self.styles['JapaneseHeading']))
        
        try:
            # å…ƒãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ç›´æ¥ç”»åƒã‚’ç”Ÿæˆ
            img_path = self._create_image_from_original_data(spectrum_data, file_key)
            
            if img_path and os.path.exists(img_path):
                try:
                    img = Image(img_path, width=7*inch, height=5.6*inch)
                    content.append(img)
                    content.append(Spacer(1, 0.2*inch))
                    st.success("âœ… å…ƒã®ã‚¹ãƒšã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ç›´æ¥ã‚°ãƒ©ãƒ•ã‚’ç”Ÿæˆã—ã¾ã—ãŸ")
                except Exception as img_error:
                    st.warning(f"ç”»åƒè¿½åŠ ã‚¨ãƒ©ãƒ¼: {img_error}")
                    content.append(self._create_text_based_spectrum_info(spectrum_data))
            else:
                content.append(self._create_text_based_spectrum_info(spectrum_data))
            
        except Exception as e:
            st.error(f"å…ƒãƒ‡ãƒ¼ã‚¿ã‚°ãƒ©ãƒ•ä½œæˆã‚¨ãƒ©ãƒ¼: {e}")
            content.append(self._create_text_based_spectrum_info(spectrum_data))
        
        # ã‚°ãƒ©ãƒ•ã®èª¬æ˜
        if self.japanese_font_available:
            description = """
            ä¸Šå›³ã¯å…ƒã®ã‚¹ãƒšã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ç›´æ¥ç”Ÿæˆã•ã‚ŒãŸãƒ©ãƒãƒ³ã‚¹ãƒšã‚¯ãƒˆãƒ«ã¨ãƒ”ãƒ¼ã‚¯æ¤œå‡ºçµæœã§ã™ã€‚
            èµ¤ã„ç‚¹ã¯æ¤œå‡ºã•ã‚ŒãŸãƒ”ãƒ¼ã‚¯ã€ç·‘ã®æ˜Ÿå°ã¯æ‰‹å‹•ã§è¿½åŠ ã•ã‚ŒãŸãƒ”ãƒ¼ã‚¯ã‚’è¡¨ç¤ºã—ã¦ã„ã¾ã™ã€‚
            """
        else:
            description = """
            The above figure shows the Raman spectrum and peak detection results generated directly from original data.
            Red points indicate detected peaks, green stars show manually added peaks.
            """
        
        description = self._sanitize_text_for_pdf(description)
        content.append(Paragraph(description, self.styles['JapaneseNormal']))
        content.append(Spacer(1, 0.2*inch))
        
        return content
    
    def _create_image_from_original_data(self, spectrum_data: Dict, file_key: str) -> str:
        """å…ƒã®ã‚¹ãƒšã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ç›´æ¥ç”»åƒã‚’ç”Ÿæˆ"""
        try:
            import matplotlib
            matplotlib.use('Agg')
            import matplotlib.pyplot as plt
            import numpy as np
            
            # ç”»åƒãƒ‘ã‚¹
            img_path = os.path.join(self.temp_dir, f"original_spectrum_{file_key}.png")
            
            # ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
            wavenum = spectrum_data.get('wavenum', [])
            spectrum = spectrum_data.get('spectrum', [])
            second_derivative = spectrum_data.get('second_derivative', [])
            detected_peaks = spectrum_data.get('detected_peaks', [])
            detected_prominences = spectrum_data.get('detected_prominences', [])
            manual_peaks = spectrum_data.get('manual_peaks', [])
            excluded_peaks = spectrum_data.get('excluded_peaks', set())
            all_peaks = spectrum_data.get('all_peaks', [])
            all_prominences = spectrum_data.get('all_prominences', [])
            
            if not wavenum or not spectrum:
                raise Exception("åŸºæœ¬ã‚¹ãƒšã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã¾ã™")
            
            # numpyé…åˆ—ã«å¤‰æ›
            wavenum = np.array(wavenum)
            spectrum = np.array(spectrum)
            if len(second_derivative) > 0:
                second_derivative = np.array(second_derivative)
            
            # å›³ã‚’ä½œæˆ
            fig, axes = plt.subplots(3, 1, figsize=(10, 8), facecolor='white')
            fig.suptitle('Raman Spectrum Analysis', fontsize=14, y=0.95)
            
            # 1æ®µç›®ï¼šãƒ¡ã‚¤ãƒ³ã‚¹ãƒšã‚¯ãƒˆãƒ«
            axes[0].plot(wavenum, spectrum, 'b-', linewidth=2, label='Spectrum')
            
            # æœ‰åŠ¹ãªæ¤œå‡ºãƒ”ãƒ¼ã‚¯
            valid_peaks = [i for i in detected_peaks if i not in excluded_peaks]
            if len(valid_peaks) > 0 and len(valid_peaks) <= len(wavenum):
                valid_indices = [i for i in valid_peaks if 0 <= i < len(wavenum)]
                if valid_indices:
                    axes[0].scatter(wavenum[valid_indices], spectrum[valid_indices], 
                                   c='red', s=50, label='Detected Peaks', zorder=5)
            
            # é™¤å¤–ã•ã‚ŒãŸãƒ”ãƒ¼ã‚¯
            excluded_indices = [i for i in excluded_peaks if 0 <= i < len(wavenum)]
            if excluded_indices:
                axes[0].scatter(wavenum[excluded_indices], spectrum[excluded_indices], 
                               c='gray', s=50, marker='x', label='Excluded Peaks', zorder=5)
            
            # æ‰‹å‹•ãƒ”ãƒ¼ã‚¯
            if manual_peaks:
                manual_x = []
                manual_y = []
                for peak_wn in manual_peaks:
                    # æœ€ã‚‚è¿‘ã„æ³¢æ•°ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’è¦‹ã¤ã‘ã‚‹
                    idx = np.argmin(np.abs(wavenum - peak_wn))
                    if 0 <= idx < len(spectrum):
                        manual_x.append(peak_wn)
                        manual_y.append(spectrum[idx])
                
                if manual_x:
                    axes[0].scatter(manual_x, manual_y, c='green', s=80, marker='*', 
                                   label='Manual Peaks', zorder=6)
            
            axes[0].set_ylabel('Intensity (a.u.)')
            axes[0].legend()
            axes[0].grid(True, alpha=0.3)
            
            # 2æ®µç›®ï¼š2æ¬¡å¾®åˆ†
            if len(second_derivative) > 0:
                axes[1].plot(wavenum, second_derivative, 'purple', linewidth=1, label='2nd Derivative')
                axes[1].axhline(y=0, color='black', linestyle='--', alpha=0.5)
            else:
                # 2æ¬¡å¾®åˆ†ãƒ‡ãƒ¼ã‚¿ãŒãªã„å ´åˆã¯ç°¡æ˜“è¨ˆç®—
                if len(spectrum) > 4:
                    simple_2nd_deriv = np.gradient(np.gradient(spectrum))
                    axes[1].plot(wavenum, simple_2nd_deriv, 'purple', linewidth=1, label='2nd Derivative (Calculated)')
                    axes[1].axhline(y=0, color='black', linestyle='--', alpha=0.5)
            
            axes[1].set_ylabel('2nd Derivative')
            axes[1].legend()
            axes[1].grid(True, alpha=0.3)
            
            # 3æ®µç›®ï¼šProminence
            if len(all_peaks) > 0 and len(all_prominences) > 0:
                # å…¨ãƒ”ãƒ¼ã‚¯ã®prominence
                valid_all_indices = [i for i in all_peaks if 0 <= i < len(wavenum)]
                if valid_all_indices and len(valid_all_indices) <= len(all_prominences):
                    axes[2].scatter(wavenum[valid_all_indices], all_prominences[:len(valid_all_indices)], 
                                   c='orange', s=20, alpha=0.6, label='All Peaks')
                
                # æœ‰åŠ¹ãªãƒ”ãƒ¼ã‚¯ã®prominence
                if len(valid_peaks) > 0 and len(detected_prominences) > 0:
                    valid_prom_indices = [i for i in valid_peaks if 0 <= i < len(wavenum)]
                    if valid_prom_indices:
                        # prominenceãƒ‡ãƒ¼ã‚¿ã‚’ãƒãƒƒãƒ”ãƒ³ã‚°
                        valid_prominences = []
                        for peak_idx in valid_prom_indices:
                            # detected_peaksã§ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’è¦‹ã¤ã‘ã‚‹
                            try:
                                orig_idx = list(detected_peaks).index(peak_idx)
                                if orig_idx < len(detected_prominences):
                                    valid_prominences.append(detected_prominences[orig_idx])
                                else:
                                    valid_prominences.append(0.1)  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
                            except ValueError:
                                valid_prominences.append(0.1)
                        
                        if valid_prominences:
                            axes[2].scatter(wavenum[valid_prom_indices], valid_prominences, 
                                           c='red', s=60, label='Valid Peaks', zorder=5)
            
            axes[2].set_xlabel('Wavenumber (cmâ»Â¹)')
            axes[2].set_ylabel('Prominence')
            axes[2].legend()
            axes[2].grid(True, alpha=0.3)
            
            plt.tight_layout()
            plt.savefig(img_path, dpi=150, bbox_inches='tight', facecolor='white')
            plt.close()
            
            return img_path
            
        except Exception as e:
            st.error(f"å…ƒãƒ‡ãƒ¼ã‚¿ç”»åƒç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            st.write(f"ãƒ‡ãƒãƒƒã‚°æƒ…å ±: ãƒ‡ãƒ¼ã‚¿ã‚­ãƒ¼ = {list(spectrum_data.keys())}")
            return None
    
    def _create_text_based_spectrum_info(self, spectrum_data: Dict) -> Paragraph:
        """ãƒ†ã‚­ã‚¹ãƒˆãƒ™ãƒ¼ã‚¹ã®ã‚¹ãƒšã‚¯ãƒˆãƒ«æƒ…å ±ã‚’ä½œæˆ"""
        try:
            wavenum = spectrum_data.get('wavenum', [])
            spectrum = spectrum_data.get('spectrum', [])
            detected_peaks = spectrum_data.get('detected_peaks', [])
            manual_peaks = spectrum_data.get('manual_peaks', [])
            
            info_text = f"""
            ã‚¹ãƒšã‚¯ãƒˆãƒ«æƒ…å ± (ãƒ†ã‚­ã‚¹ãƒˆè¡¨ç¤º):
            â€¢ ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆæ•°: {len(wavenum)}
            â€¢ æ³¢æ•°ç¯„å›²: {min(wavenum):.1f} - {max(wavenum):.1f} cmâ»Â¹
            â€¢ å¼·åº¦ç¯„å›²: {min(spectrum):.3f} - {max(spectrum):.3f}
            â€¢ æ¤œå‡ºãƒ”ãƒ¼ã‚¯æ•°: {len(detected_peaks)}
            â€¢ æ‰‹å‹•ãƒ”ãƒ¼ã‚¯æ•°: {len(manual_peaks)}
            
            æ³¨: ã‚°ãƒ©ãƒ•ã‚£ã‚«ãƒ«è¡¨ç¤ºã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸãŸã‚ã€ãƒ†ã‚­ã‚¹ãƒˆå½¢å¼ã§è¡¨ç¤ºã—ã¦ã„ã¾ã™ã€‚
            """
            
            info_text = self._sanitize_text_for_pdf(info_text)
            return Paragraph(info_text, self.styles['JapaneseNormal'])
            
        except Exception as e:
            error_text = self._sanitize_text_for_pdf(f"ã‚¹ãƒšã‚¯ãƒˆãƒ«æƒ…å ±è¡¨ç¤ºã‚¨ãƒ©ãƒ¼: {e}")
            return Paragraph(error_text, self.styles['JapaneseNormal'])
    
    
    def _create_title_page(self, file_key: str) -> List:
        """ã‚¿ã‚¤ãƒˆãƒ«ãƒšãƒ¼ã‚¸ã‚’ä½œæˆ"""
        content = []
        
        # ãƒ¡ã‚¤ãƒ³ã‚¿ã‚¤ãƒˆãƒ«
        title = Paragraph(
            "ãƒ©ãƒãƒ³ã‚¹ãƒšã‚¯ãƒˆãƒ«è§£æãƒ¬ãƒãƒ¼ãƒˆ",
            self.styles['JapaneseTitle']
        )
        content.append(title)
        content.append(Spacer(1, 0.5*inch))
        
        # ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±
        file_info = f"""
        <b>è§£æå¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«:</b> {file_key}<br/>
        <b>ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆæ—¥æ™‚:</b> {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %Hæ™‚%Måˆ†')}<br/>
        <b>ã‚·ã‚¹ãƒ†ãƒ :</b> RamanEye AI Analysis System<br/>
        <b>ãƒãƒ¼ã‚¸ãƒ§ãƒ³:</b> 2.0 (Enhanced Security Edition)
        """
        
        content.append(Paragraph(file_info, self.styles['JapaneseNormal']))
        content.append(Spacer(1, 0.5*inch))
        
        # å…è²¬äº‹é …
        disclaimer = """
        <b>ã€é‡è¦ã€‘æœ¬ãƒ¬ãƒãƒ¼ãƒˆã«ã¤ã„ã¦</b><br/>
        æœ¬ãƒ¬ãƒãƒ¼ãƒˆã¯AIã«ã‚ˆã‚‹è‡ªå‹•è§£æçµæœã‚’å«ã‚“ã§ã„ã¾ã™ã€‚
        çµæœã®è§£é‡ˆãŠã‚ˆã³æ´»ç”¨ã«ã¤ã„ã¦ã¯ã€å°‚é–€å®¶ã«ã‚ˆã‚‹æ¤œè¨¼ã‚’æ¨å¥¨ã—ã¾ã™ã€‚
        æ¸¬å®šæ¡ä»¶ã€ã‚µãƒ³ãƒ—ãƒ«å‰å‡¦ç†ã€è£…ç½®è¼ƒæ­£ç­‰ã®è¦å› ãŒçµæœã«å½±éŸ¿ã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚
        """
        
        content.append(Paragraph(disclaimer, self.styles['JapaneseNormal']))
        content.append(PageBreak())
        
        return content
    
    def _create_executive_summary(self, peak_data: List[Dict], analysis_result: str) -> List:
        """å®Ÿè¡Œã‚µãƒãƒªãƒ¼ã‚’ä½œæˆ"""
        content = []
        
        content.append(Paragraph("å®Ÿè¡Œã‚µãƒãƒªãƒ¼", self.styles['JapaneseHeading']))
        
        # ãƒ”ãƒ¼ã‚¯çµ±è¨ˆ
        total_peaks = len(peak_data)
        auto_peaks = len([p for p in peak_data if p.get('type') == 'auto'])
        manual_peaks = len([p for p in peak_data if p.get('type') == 'manual'])
        
        summary_text = f"""
        <b>æ¤œå‡ºãƒ”ãƒ¼ã‚¯ç·æ•°:</b> {total_peaks}<br/>
        <b>è‡ªå‹•æ¤œå‡º:</b> {auto_peaks} ãƒ”ãƒ¼ã‚¯<br/>
        <b>æ‰‹å‹•è¿½åŠ :</b> {manual_peaks} ãƒ”ãƒ¼ã‚¯<br/>
        <br/>
        <b>ä¸»è¦æ¤œå‡ºç¯„å›²:</b> {min([p['wavenumber'] for p in peak_data]):.0f} - {max([p['wavenumber'] for p in peak_data]):.0f} cmâ»Â¹
        """
        
        content.append(Paragraph(summary_text, self.styles['JapaneseNormal']))
        content.append(Spacer(1, 0.3*inch))
        
        # AIè§£æçµæœã®è¦ç´„ï¼ˆæœ€åˆã®200æ–‡å­—ï¼‰
        analysis_summary = analysis_result[:200] + "..." if len(analysis_result) > 200 else analysis_result
        content.append(Paragraph("<b>AIè§£æçµæœè¦ç´„:</b>", self.styles['JapaneseNormal']))
        content.append(Paragraph(analysis_summary, self.styles['JapaneseNormal']))
        content.append(Spacer(1, 0.2*inch))
        
        return content
    
    def _create_graph_section(self, plotly_figure: go.Figure, file_key: str) -> List:
        """ã‚°ãƒ©ãƒ•ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’ä½œæˆ"""
        content = []
        
        content.append(Paragraph("ã‚¹ãƒšã‚¯ãƒˆãƒ«ãŠã‚ˆã³ãƒ”ãƒ¼ã‚¯æ¤œå‡ºçµæœ", self.styles['JapaneseHeading']))
        
        try:
            # Plotlyã‚°ãƒ©ãƒ•ã‚’ç”»åƒã«å¤‰æ›
            img_path = self.plotly_to_image(plotly_figure, f"spectrum_{file_key}", width=1000, height=800)
            
            # ç”»åƒã‚’PDFã«è¿½åŠ 
            if os.path.exists(img_path):
                img = Image(img_path, width=7*inch, height=5.6*inch)
                content.append(img)
                content.append(Spacer(1, 0.2*inch))
            
        except Exception as e:
            error_text = f"ã‚°ãƒ©ãƒ•è¡¨ç¤ºã‚¨ãƒ©ãƒ¼: {e}"
            content.append(Paragraph(error_text, self.styles['JapaneseNormal']))
        
        # ã‚°ãƒ©ãƒ•ã®èª¬æ˜
        graph_description = """
        ä¸Šå›³ã¯ãƒ©ãƒãƒ³ã‚¹ãƒšã‚¯ãƒˆãƒ«ã¨ãƒ”ãƒ¼ã‚¯æ¤œå‡ºçµæœã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚
        èµ¤ã„ç‚¹ã¯æ¤œå‡ºã•ã‚ŒãŸãƒ”ãƒ¼ã‚¯ã€ç·‘ã®æ˜Ÿå°ã¯æ‰‹å‹•ã§è¿½åŠ ã•ã‚ŒãŸãƒ”ãƒ¼ã‚¯ã‚’è¡¨ç¤ºã—ã¦ã„ã¾ã™ã€‚
        ä¸‹éƒ¨ã®ãƒ—ãƒ­ãƒƒãƒˆã¯2æ¬¡å¾®åˆ†ã‚¹ãƒšã‚¯ãƒˆãƒ«ã¨ãƒ”ãƒ¼ã‚¯ã®Prominenceå€¤ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚
        """
        
        content.append(Paragraph(graph_description, self.styles['JapaneseNormal']))
        content.append(Spacer(1, 0.2*inch))
        
        return content
    
    def _create_peak_details_section(self, peak_summary_df: pd.DataFrame, peak_data: List[Dict]) -> List:
        """ãƒ”ãƒ¼ã‚¯è©³ç´°ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’ä½œæˆ"""
        content = []
        
        content.append(Paragraph("æ¤œå‡ºãƒ”ãƒ¼ã‚¯è©³ç´°", self.styles['JapaneseHeading']))
        
        # DataFrameã‚’ãƒ†ãƒ¼ãƒ–ãƒ«ã«å¤‰æ›
        table_data = [peak_summary_df.columns.tolist()]  # ãƒ˜ãƒƒãƒ€ãƒ¼
        for _, row in peak_summary_df.iterrows():
            table_data.append(row.tolist())
        
        # ãƒ†ãƒ¼ãƒ–ãƒ«ã‚¹ã‚¿ã‚¤ãƒ«ã®è¨­å®š
        table = Table(table_data, colWidths=[1*inch, 1.5*inch, 1.2*inch, 1.2*inch, 1.5*inch])
        table.setStyle(TableStyle([
            ('BACKGROUND', (0, 0), (-1, 0), colors.grey),
            ('TEXTCOLOR', (0, 0), (-1, 0), colors.whitesmoke),
            ('ALIGN', (0, 0), (-1, -1), 'CENTER'),
            ('FONTNAME', (0, 0), (-1, 0), self.japanese_font_name),
            ('FONTSIZE', (0, 0), (-1, 0), 10),
            ('BOTTOMPADDING', (0, 0), (-1, 0), 12),
            ('BACKGROUND', (0, 1), (-1, -1), colors.beige),
            ('FONTNAME', (0, 1), (-1, -1), self.japanese_font_name),
            ('FONTSIZE', (0, 1), (-1, -1), 9),
            ('GRID', (0, 0), (-1, -1), 1, colors.black)
        ]))
        
        content.append(table)
        content.append(Spacer(1, 0.3*inch))
        
        return content
    
    def _create_ai_analysis_section(self, analysis_result: str) -> List:
        """AIè§£æçµæœã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’ä½œæˆ"""
        content = []
        
        content.append(Paragraph("AIè§£æçµæœ", self.styles['JapaneseHeading']))
        
        # é•·ã„ãƒ†ã‚­ã‚¹ãƒˆã‚’æ®µè½ã«åˆ†å‰²
        paragraphs = analysis_result.split('\n\n')
        
        for para in paragraphs:
            if para.strip():
                # ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³å½¢å¼ã®ç°¡å˜ãªå¤‰æ›
                para = para.replace('**', '<b>').replace('**', '</b>')
                para = para.replace('*', '<i>').replace('*', '</i>')
                
                content.append(Paragraph(para.strip(), self.styles['JapaneseNormal']))
                content.append(Spacer(1, 0.1*inch))
        
        return content
    
    def _create_references_section(self, relevant_docs: List[Dict]) -> List:
        """å‚è€ƒæ–‡çŒ®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’ä½œæˆ"""
        content = []
        
        content.append(PageBreak())
        content.append(Paragraph("å‚è€ƒæ–‡çŒ®", self.styles['JapaneseHeading']))
        
        for i, doc in enumerate(relevant_docs, 1):
            filename = doc.get('metadata', {}).get('filename', f'æ–‡çŒ®{i}')
            similarity = doc.get('similarity_score', 0.0)
            preview = doc.get('text', '')[:200] + "..." if len(doc.get('text', '')) > 200 else doc.get('text', '')
            
            ref_text = f"""
            <b>{i}. {filename}</b><br/>
            é¡ä¼¼åº¦: {similarity:.3f}<br/>
            å†…å®¹æŠœç²‹: {preview}<br/>
            """
            
            content.append(Paragraph(ref_text, self.styles['JapaneseNormal']))
            content.append(Spacer(1, 0.2*inch))
        
        return content
    
    def _create_additional_info_section(self, user_hint: str) -> List:
        """è£œè¶³æƒ…å ±ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’ä½œæˆ"""
        content = []
        
        content.append(Paragraph("è£œè¶³æƒ…å ±", self.styles['JapaneseHeading']))
        content.append(Paragraph(f"ãƒ¦ãƒ¼ã‚¶ãƒ¼æä¾›ãƒ’ãƒ³ãƒˆ: {user_hint}", self.styles['JapaneseNormal']))
        content.append(Spacer(1, 0.2*inch))
        
        return content
    
    def _create_qa_section(self, qa_history: List[Dict]) -> List:
        """Q&Aã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’ä½œæˆ"""
        content = []
        
        content.append(PageBreak())
        content.append(Paragraph("è³ªå•å¿œç­”å±¥æ­´", self.styles['JapaneseHeading']))
        
        for i, qa in enumerate(qa_history, 1):
            qa_text = f"""
            <b>è³ªå•{i}:</b> {qa['question']}<br/>
            <b>å›ç­”{i}:</b> {qa['answer']}<br/>
            <i>æ—¥æ™‚: {qa['timestamp']}</i><br/>
            """
            
            content.append(Paragraph(qa_text, self.styles['JapaneseNormal']))
            content.append(Spacer(1, 0.2*inch))
        
        return content
    
    def _create_appendix_section(self) -> List:
        """ä»˜éŒ²ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’ä½œæˆ"""
        content = []
        
        content.append(PageBreak())
        content.append(Paragraph("ä»˜éŒ²", self.styles['JapaneseHeading']))
        
        # ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±
        system_info = f"""
        <b>ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±:</b><br/>
        ç”Ÿæˆæ—¥æ™‚: {datetime.now().isoformat()}<br/>
        ãƒ¬ãƒãƒ¼ãƒˆå½¢å¼: PDF (ReportLabç”Ÿæˆ)<br/>
        AIåˆ†æã‚¨ãƒ³ã‚¸ãƒ³: OpenAI GPT Model<br/>
        ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£æ©Ÿèƒ½: {'æœ‰åŠ¹' if SECURITY_AVAILABLE else 'ç„¡åŠ¹'}<br/>
        """
        
        content.append(Paragraph(system_info, self.styles['JapaneseNormal']))
        
        return content
    
    def cleanup_temp_files(self):
        """ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        try:
            import shutil
            if os.path.exists(self.temp_dir):
                shutil.rmtree(self.temp_dir)
        except Exception as e:
            st.warning(f"ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚¨ãƒ©ãƒ¼: {e}")

def render_qa_section(file_key, analysis_context, llm_connector):
    """AIè§£æçµæœã®å¾Œã«è³ªå•å¿œç­”ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¡¨ç¤ºã™ã‚‹é–¢æ•°"""
    qa_history_key = f"{file_key}_qa_history"
    if qa_history_key not in st.session_state:
        st.session_state[qa_history_key] = []
    
    st.markdown("---")
    st.subheader(f"ğŸ’¬ è¿½åŠ è³ªå• - {file_key}")
    
    # è³ªå•å±¥æ­´ã®è¡¨ç¤º
    if st.session_state[qa_history_key]:
        with st.expander("ğŸ“š è³ªå•å±¥æ­´ã‚’è¡¨ç¤º", expanded=False):
            for i, qa in enumerate(st.session_state[qa_history_key], 1):
                st.markdown(f"**è³ªå•{i}:** {qa['question']}")
                st.markdown(f"**å›ç­”{i}:** {qa['answer']}")
                st.markdown(f"*è³ªå•æ—¥æ™‚: {qa['timestamp']}*")
                st.markdown("---")
    
    # è³ªå•å…¥åŠ›ãƒ•ã‚©ãƒ¼ãƒ 
    with st.form(key=f"qa_form_{file_key}"):
        st.markdown("**è§£æçµæœã«ã¤ã„ã¦è³ªå•ãŒã‚ã‚Œã°ã€ä¸‹è¨˜ã«ã”è¨˜å…¥ãã ã•ã„ï¼š**")
        
        st.markdown("""
        **è³ªå•ä¾‹:**
        - ã“ã®ãƒ”ãƒ¼ã‚¯ã¯ä½•ã«ç”±æ¥ã—ã¾ã™ã‹ï¼Ÿ
        - ä»–ã®å¯èƒ½æ€§ã®ã‚ã‚‹ç‰©è³ªã¯ã‚ã‚Šã¾ã™ã‹ï¼Ÿ
        - æ¸¬å®šæ¡ä»¶ã§æ³¨æ„ã™ã¹ãç‚¹ã¯ï¼Ÿ
        - å®šé‡åˆ†æã¯å¯èƒ½ã§ã™ã‹ï¼Ÿ
        """)
        
        user_question = st.text_area(
            "è³ªå•å†…å®¹:",
            placeholder="ä¾‹: 1500 cmâ»Â¹ä»˜è¿‘ã®ãƒ”ãƒ¼ã‚¯ã«ã¤ã„ã¦è©³ã—ãæ•™ãˆã¦ãã ã•ã„",
            height=100
        )
        
        submit_button = st.form_submit_button("ğŸ’¬ è³ªå•ã™ã‚‹")
    
    # è³ªå•å‡¦ç†
    if submit_button and user_question.strip():
        with st.spinner("AIãŒå›ç­”ã‚’è€ƒãˆã¦ã„ã¾ã™..."):
            try:
                answer = llm_connector.generate_qa_response(
                    question=user_question,
                    context=analysis_context,
                    previous_qa_history=st.session_state[qa_history_key]
                )
                
                new_qa = {
                    'question': user_question,
                    'answer': answer,
                    'timestamp': datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                }
                st.session_state[qa_history_key].append(new_qa)
                
                # st.success("âœ… å›ç­”ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
                
            except Exception as e:
                st.error(f"è³ªå•å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
    
    elif submit_button and not user_question.strip():
        st.warning("è³ªå•å†…å®¹ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
        
def peak_ai_analysis_mode():
    """å¼·åŒ–ã•ã‚ŒãŸPeak AI analysis mode"""
    if not PDF_AVAILABLE:
        st.error("AIè§£ææ©Ÿèƒ½ã‚’ä½¿ç”¨ã™ã‚‹ã«ã¯ã€ä»¥ä¸‹ã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒå¿…è¦ã§ã™ï¼š")
        st.code("pip install PyPDF2 python-docx openai faiss-cpu sentence-transformers")
        return
    
    st.header("ãƒ©ãƒãƒ³ãƒ”ãƒ¼ã‚¯AIè§£æ")
    
    # LLMæ¥ç¶šè¨­å®šï¼ˆã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£å¼·åŒ–ç‰ˆï¼‰
    llm_connector = LLMConnector()
    
    # OpenAI APIè¨­å®š
    llm_ready = llm_connector.setup_llm_connection()
    
    # RAGè¨­å®šã‚»ã‚¯ã‚·ãƒ§ãƒ³ï¼ˆã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£å¼·åŒ–ç‰ˆï¼‰
    st.sidebar.subheader("ğŸ“š è«–æ–‡ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹è¨­å®š")
    
    # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ“ä½œãƒ¢ãƒ¼ãƒ‰ã®é¸æŠ
    db_mode = st.sidebar.radio(
        "ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ“ä½œãƒ¢ãƒ¼ãƒ‰",
        ["æ–°è¦ä½œæˆ", "æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹èª­ã¿è¾¼ã¿"],
        index=0
    )
     
    # ä¸€æ™‚ä¿å­˜ç”¨ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
    TEMP_DIR = "./tmp_uploads"
    os.makedirs(TEMP_DIR, exist_ok=True)
    
    # RAGã‚·ã‚¹ãƒ†ãƒ ã®åˆæœŸåŒ–ï¼ˆã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£å¼·åŒ–ç‰ˆï¼‰
    if 'rag_system' not in st.session_state:
        st.session_state.rag_system = RamanRAGSystem()
        st.session_state.rag_db_built = False
    
    if db_mode == "æ–°è¦ä½œæˆ":
        setup_new_database(TEMP_DIR)
    elif db_mode == "æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹èª­ã¿è¾¼ã¿":
        load_existing_database()
    
    # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹çŠ¶æ…‹è¡¨ç¤º
    if st.session_state.rag_db_built:
        st.sidebar.success("âœ… è«–æ–‡ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ§‹ç¯‰æ¸ˆã¿")
        
        if st.sidebar.button("ğŸ“Š ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æƒ…å ±ã‚’è¡¨ç¤º"):
            db_info = st.session_state.rag_system.get_database_info()
            st.sidebar.json(db_info)
    else:
        st.sidebar.info("â„¹ï¸ è«–æ–‡ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æœªæ§‹ç¯‰")
        
    # ã‚µã‚¤ãƒ‰ãƒãƒ¼ã«è£œè¶³æŒ‡ç¤ºæ¬„ã‚’è¿½åŠ 
    user_hint = st.sidebar.text_area(
        "AIã¸ã®è£œè¶³ãƒ’ãƒ³ãƒˆï¼ˆä»»æ„ï¼‰",
        placeholder="ä¾‹ï¼šã“ã®è©¦æ–™ã¯ãƒãƒªã‚¨ãƒãƒ¬ãƒ³ç³»é«˜åˆ†å­ã§ã‚ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ã€ãªã©"
    )
    
    # ãƒ”ãƒ¼ã‚¯è§£æéƒ¨åˆ†ã®å®Ÿè¡Œï¼ˆã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£å¼·åŒ–ç‰ˆï¼‰
    perform_peak_analysis_with_ai(llm_connector, user_hint, llm_ready)

def setup_new_database(TEMP_DIR):
    """æ–°è¦ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®ä½œæˆ"""
    uploaded_files = st.sidebar.file_uploader(
        "ğŸ“„ æ–‡çŒ®PDFã‚’é¸æŠã—ã¦ãã ã•ã„ï¼ˆè¤‡æ•°å¯ï¼‰",
        type=["pdf", "docx", "txt"],
        accept_multiple_files=True
    )

    if st.sidebar.button("ğŸ“š è«–æ–‡ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ§‹ç¯‰"):
        if not uploaded_files:
            st.sidebar.warning("æ–‡çŒ®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚")
        else:
            with st.spinner("æ–‡çŒ®ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã€ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’æ§‹ç¯‰ä¸­..."):
                security_manager = get_security_manager() if SECURITY_AVAILABLE else None
                current_user = st.session_state.get('current_user', {})
                user_id = current_user.get('username', 'unknown')
                
                uploaded_count = 0
                for uploaded_file in uploaded_files:
                    try:
                        # ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£å¼·åŒ–ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
                        if security_manager:
                            upload_result = security_manager.secure_file_upload(uploaded_file, user_id)
                            if upload_result['status'] == 'success':
                                uploaded_count += 1
                            else:
                                st.error(f"ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {upload_result['message']}")
                        else:
                            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: åŸºæœ¬çš„ãªãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
                            # uploaded_fileãŒstreamlitã®UploadedFileã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆãªã®ã§nameå±æ€§ã‚’ä½¿ç”¨
                            save_path = os.path.join(TEMP_DIR, uploaded_file.name)
                            with open(save_path, "wb") as f:
                                f.write(uploaded_file.getbuffer())
                            uploaded_count += 1
                    except Exception as e:
                        st.error(f"ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ã‚¨ãƒ©ãƒ¼ ({uploaded_file.name}): {e}")
                
                if uploaded_count > 0:
                    st.session_state.rag_system.build_vector_database(TEMP_DIR)
                    st.session_state.rag_db_built = True
                    st.sidebar.success(f"âœ… {uploaded_count} ä»¶ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’æ§‹ç¯‰ã—ã¾ã—ãŸã€‚")

def load_existing_database():
    """æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®èª­ã¿è¾¼ã¿"""
    st.sidebar.markdown("---")
    st.sidebar.subheader("ğŸ“‚ æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹èª­ã¿è¾¼ã¿")
    st.sidebar.info("ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£æ©Ÿèƒ½ã«ã‚ˆã‚Šã€ã‚¢ã‚¯ã‚»ã‚¹æ¨©é™ã®ã‚ã‚‹ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®ã¿èª­ã¿è¾¼ã¿å¯èƒ½ã§ã™ã€‚")

def perform_peak_analysis_with_ai(llm_connector, user_hint, llm_ready):
    """ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£å¼·åŒ–ã•ã‚ŒãŸAIæ©Ÿèƒ½ã‚’å«ã‚€ãƒ”ãƒ¼ã‚¯è§£æã®å®Ÿè¡Œ"""
    # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®š
    pre_start_wavenum = 400
    pre_end_wavenum = 2000
    
    # ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆã®åˆæœŸåŒ–
    for key, default in {
        "prominence_threshold": 100,
        "second_deriv_threshold": 100,
        "savgol_wsize": 5,
        "spectrum_type_select": "ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³å‰Šé™¤",
        "second_deriv_smooth": 5,
        "manual_peak_keys": []
    }.items():
        if key not in st.session_state:
            st.session_state[key] = default
    
    # UIãƒ‘ãƒãƒ«ï¼ˆSidebarï¼‰
    start_wavenum = st.sidebar.number_input("æ³¢æ•°ï¼ˆé–‹å§‹ï¼‰ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„:", -200, 4800, value=pre_start_wavenum, step=100)
    end_wavenum = st.sidebar.number_input("æ³¢æ•°ï¼ˆçµ‚äº†ï¼‰ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„:", -200, 4800, value=pre_end_wavenum, step=100)
    dssn_th = st.sidebar.number_input("ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„:", 1, 10000, value=1000, step=1) / 1e7
    savgol_wsize = st.sidebar.number_input("ç§»å‹•å¹³å‡ã®ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚µã‚¤ã‚ºã‚’å…¥åŠ›ã—ã¦ãã ã•ã„:", 5, 101, step=2, key="savgol_wsize")
    
    st.sidebar.subheader("ãƒ”ãƒ¼ã‚¯æ¤œå‡ºè¨­å®š")
    
    spectrum_type = st.sidebar.selectbox("è§£æã‚¹ãƒšã‚¯ãƒˆãƒ«:", ["ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³å‰Šé™¤", "ç§»å‹•å¹³å‡å¾Œ"], index=0, key="spectrum_type_select")
    
    second_deriv_smooth = st.sidebar.number_input(
        "2æ¬¡å¾®åˆ†å¹³æ»‘åŒ–:", 3, 35,
        step=2, key="second_deriv_smooth"
    )
    
    second_deriv_threshold = st.sidebar.number_input(
        "2æ¬¡å¾®åˆ†é–¾å€¤:",
        min_value=0,
        max_value=1000,
        step=10,
        key="second_deriv_threshold"
    )
    
    peak_prominence_threshold = st.sidebar.number_input(
        "ãƒ”ãƒ¼ã‚¯Prominenceé–¾å€¤:",
        min_value=0,
        max_value=1000,
        step=10,
        key="prominence_threshold"
    )

    # ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
    uploaded_files = st.file_uploader(
        "ãƒ©ãƒãƒ³ã‚¹ãƒšã‚¯ãƒˆãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ï¼ˆå˜æ•°ï¼‰", 
        accept_multiple_files=False, 
        key="file_uploader",
    )
    
    # openai_api_key = os.getenv("OPENAI_API_KEY")
    # st.sidebar.write("OPENAI_API_KEY is set? ", bool(os.getenv("OPENAI_API_KEY")))
    
    # ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«å¤‰æ›´æ¤œå‡º
    if uploaded_files:
        new_filenames = [uploaded_files.name]
    else:
        new_filenames = []
    prev_filenames = st.session_state.get("uploaded_filenames", [])

    # è¨­å®šå¤‰æ›´æ¤œå‡º
    config_keys = ["spectrum_type_select", "second_deriv_smooth", "second_deriv_threshold", "prominence_threshold"]
    config_changed = any(
        st.session_state.get(f"prev_{key}") != st.session_state[key] for key in config_keys
    )
    file_changed = new_filenames != prev_filenames

    # æ‰‹å‹•ãƒ”ãƒ¼ã‚¯åˆæœŸåŒ–æ¡ä»¶
    if config_changed or file_changed:
        for key in list(st.session_state.keys()):
            if key.endswith("_manual_peaks"):
                del st.session_state[key]
        st.session_state["manual_peak_keys"] = []
        st.session_state["uploaded_filenames"] = new_filenames
        for k in config_keys:
            st.session_state[f"prev_{k}"] = st.session_state[k]
            
    file_labels = []
    all_spectra = []
    all_bsremoval_spectra = []
    all_averemoval_spectra = []
    all_wavenum = []
    
    # ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£çŠ¶æ…‹è¡¨ç¤º
    if SECURITY_AVAILABLE:
        security_manager = get_security_manager()
        if security_manager:
            security_status = security_manager.get_security_status()
            
            with st.expander("ğŸ›¡ï¸ ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ…‹", expanded=False):
                col1, col2 = st.columns(2)
                with col1:
                    st.write("**ãƒ‡ãƒ¼ã‚¿ä¿è­·æ©Ÿèƒ½:**")
                    st.write(f"ğŸ” æš—å·åŒ–: {'âœ…' if security_status['encryption_enabled'] else 'âŒ'}")
                    st.write(f"ğŸ” å®Œå…¨æ€§ãƒã‚§ãƒƒã‚¯: {'âœ…' if security_status['integrity_checking_enabled'] else 'âŒ'}")
                    st.write(f"ğŸ›¡ï¸ ã‚¢ã‚¯ã‚»ã‚¹åˆ¶å¾¡: {'âœ…' if security_status['access_control_enabled'] else 'âŒ'}")
                
                with col2:
                    st.write("**é€šä¿¡:**")
                    st.write(f"ğŸŒ HTTPSå¼·åˆ¶: {'âœ…' if security_status['https_enforced'] else 'âŒ'}")
                    st.write(f"ğŸ“ ç›£æŸ»ãƒ­ã‚°: {'âœ…' if security_status['audit_logging_enabled'] else 'âŒ'}")
                    st.write(f"ğŸ”‘ ã‚­ãƒ¼: {'âœ…' if security_status['master_key_exists'] else 'âŒ'}")
    else:
        st.warning("âš ï¸ ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ãŒç„¡åŠ¹ã§ã™ã€‚åŸºæœ¬æ©Ÿèƒ½ã®ã¿å‹•ä½œã—ã¾ã™ã€‚")
    
    # ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ­ã‚°è¨˜éŒ²
    if SECURITY_AVAILABLE:
        security_manager = get_security_manager()
        current_user = st.session_state.get('current_user', {})
        user_id = current_user.get('username', 'unknown')
        
        if security_manager:
            security_manager.audit_logger.log_security_event(
                event_type="PEAK_ANALYSIS_START",
                user_id=user_id,
                details={
                    'llm_ready': llm_ready,
                    'user_hint_provided': bool(user_hint),
                    'timestamp': datetime.now().isoformat()
                },
                severity="INFO"
            )
    
    if uploaded_files:
        # ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†
        try:
            result = process_spectrum_file(
                uploaded_files, start_wavenum, end_wavenum, dssn_th, savgol_wsize
            )
            wavenum, spectra, BSremoval_specta_pos, Averemoval_specta_pos, file_type, file_name = result
            
            if wavenum is None:
                st.error(f"{file_name}ã®å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ")
                return
            
            st.write(f"ãƒ•ã‚¡ã‚¤ãƒ«ã‚¿ã‚¤ãƒ—: {file_type} - {file_name}")
            
            file_labels.append(file_name)
            all_wavenum.append(wavenum)
            all_spectra.append(spectra)
            all_bsremoval_spectra.append(BSremoval_specta_pos)
            all_averemoval_spectra.append(Averemoval_specta_pos)
            
        except Exception as e:
            st.error(f"{uploaded_files.name}ã®å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        
        # ãƒ”ãƒ¼ã‚¯æ¤œå‡ºã®å®Ÿè¡Œ
        if 'peak_detection_triggered' not in st.session_state:
            st.session_state['peak_detection_triggered'] = False
    
        if st.button("ãƒ”ãƒ¼ã‚¯æ¤œå‡ºã‚’å®Ÿè¡Œ"):
            st.session_state['peak_detection_triggered'] = True
        
        if st.session_state['peak_detection_triggered']:
            perform_peak_detection_and_ai_analysis(
                file_labels, all_wavenum, all_bsremoval_spectra, all_averemoval_spectra,
                spectrum_type, second_deriv_smooth, second_deriv_threshold, peak_prominence_threshold,
                llm_connector, user_hint, llm_ready
            )
    
    # ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£æƒ…å ±è¡¨ç¤º
    st.info("ğŸ”’ ã“ã®ãƒ¢ãƒ¼ãƒ‰ã§ã¯ã€å…¨ã¦ã®ãƒ•ã‚¡ã‚¤ãƒ«æ“ä½œã¨APIé€šä¿¡ãŒå®‰å…¨ã«å®Ÿè¡Œã•ã‚Œã¾ã™ã€‚")

def perform_peak_detection_and_ai_analysis(file_labels, all_wavenum, all_bsremoval_spectra, all_averemoval_spectra,
                                          spectrum_type, second_deriv_smooth, second_deriv_threshold, peak_prominence_threshold,
                                          llm_connector, user_hint, llm_ready):
    """ãƒ”ãƒ¼ã‚¯æ¤œå‡ºã¨AIè§£æã‚’å®Ÿè¡Œ"""
    st.subheader("ãƒ”ãƒ¼ã‚¯æ¤œå‡ºçµæœ")
    
    peak_results = []
    
    # ç¾åœ¨ã®è¨­å®šã‚’è¡¨ç¤º
    st.info(f"""
    **æ¤œå‡ºè¨­å®š:**
    - ã‚¹ãƒšã‚¯ãƒˆãƒ«ã‚¿ã‚¤ãƒ—: {spectrum_type}
    - 2æ¬¡å¾®åˆ†å¹³æ»‘åŒ–: {second_deriv_smooth}, é–¾å€¤: {second_deriv_threshold} (ãƒ”ãƒ¼ã‚¯æ¤œå‡ºç”¨)
    - ãƒ”ãƒ¼ã‚¯å“ç«‹åº¦é–¾å€¤: {peak_prominence_threshold}
    """)
    
    # ãƒ”ãƒ¼ã‚¯æ¤œå‡ºã®å®Ÿè¡Œ
    for i, file_name in enumerate(file_labels):
        if spectrum_type == "ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³å‰Šé™¤":
            selected_spectrum = all_bsremoval_spectra[i]
        else:
            selected_spectrum = all_averemoval_spectra[i]
        
        wavenum = all_wavenum[i]
        
        # 2æ¬¡å¾®åˆ†è¨ˆç®—
        if len(selected_spectrum) > second_deriv_smooth:
            second_derivative = savgol_filter(selected_spectrum, int(second_deriv_smooth), 2, deriv=2)
        else:
            second_derivative = np.gradient(np.gradient(selected_spectrum))
        
        # ãƒ”ãƒ¼ã‚¯æ¤œå‡º
        peaks, properties = find_peaks(-second_derivative, height=second_deriv_threshold)
        all_peaks, properties = find_peaks(-second_derivative)

        if len(peaks) > 0:
            prominences = peak_prominences(-second_derivative, peaks)[0]
            all_prominences = peak_prominences(-second_derivative, all_peaks)[0]

            # Prominenceé–¾å€¤ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
            mask = prominences > peak_prominence_threshold
            filtered_peaks = peaks[mask]
            filtered_prominences = prominences[mask]
            
            # ãƒ”ãƒ¼ã‚¯ä½ç½®ã®è£œæ­£
            corrected_peaks = []
            corrected_prominences = []
            
            for peak_idx, prom in zip(filtered_peaks, filtered_prominences):
                window_start = max(0, peak_idx - 2)
                window_end = min(len(selected_spectrum), peak_idx + 3)
                local_window = selected_spectrum[window_start:window_end]
                
                local_max_idx = np.argmax(local_window)
                corrected_idx = window_start + local_max_idx
            
                corrected_peaks.append(corrected_idx)
                
                # è­¦å‘Šã‚’æŠ‘åˆ¶ã—ã¦prominenceè¨ˆç®—
                try:
                    with warnings.catch_warnings():
                        warnings.simplefilter("ignore")
                        local_prom_values = peak_prominences(-second_derivative, [corrected_idx])
                        local_prom = local_prom_values[0][0] if len(local_prom_values[0]) > 0 else prom
                        # prominenceå€¤ãŒ0ã¾ãŸã¯è² ã®å ´åˆã¯å…ƒã®å€¤ã‚’ä½¿ç”¨
                        if local_prom <= 0:
                            local_prom = max(0.001, prom)
                        corrected_prominences.append(local_prom)
                except Exception:
                    # ã‚¨ãƒ©ãƒ¼æ™‚ã¯å…ƒã®prominenceå€¤ã‚’ä½¿ç”¨
                    corrected_prominences.append(max(0.001, prom))
            
            filtered_peaks = np.array(corrected_peaks)
            filtered_prominences = np.array(corrected_prominences)
        else:
            filtered_peaks = np.array([])
            filtered_prominences = np.array([])
        
        # çµæœã‚’ä¿å­˜
        peak_data = {
            'file_name': file_name,
            'detected_peaks': filtered_peaks,
            'detected_prominences': filtered_prominences,
            'wavenum': wavenum,
            'spectrum': selected_spectrum,
            'second_derivative': second_derivative,
            'second_deriv_smooth': second_deriv_smooth,
            'second_deriv_threshold': second_deriv_threshold,
            'prominence_threshold': peak_prominence_threshold,
            'all_peaks': all_peaks,
            'all_prominences': all_prominences,
        }
        peak_results.append(peak_data)
        
        # çµæœã‚’è¡¨ç¤º
        # st.write(f"**{file_name}**")
        # st.write(f"æ¤œå‡ºã•ã‚ŒãŸãƒ”ãƒ¼ã‚¯æ•°: {len(filtered_peaks)} ")
        
        # ãƒ”ãƒ¼ã‚¯æƒ…å ±ã‚’ãƒ†ãƒ¼ãƒ–ãƒ«ã§è¡¨ç¤º
        if len(filtered_peaks) > 0:
            peak_wavenums = wavenum[filtered_peaks]
            peak_intensities = selected_spectrum[filtered_peaks]
            st.write("**æ¤œå‡ºã•ã‚ŒãŸãƒ”ãƒ¼ã‚¯:**")
            peak_table = pd.DataFrame({
                'ãƒ”ãƒ¼ã‚¯ç•ªå·': range(1, len(peak_wavenums) + 1),
                'æ³¢æ•° (cmâ»Â¹)': [f"{wn:.1f}" for wn in peak_wavenums],
                'å¼·åº¦': [f"{intensity:.3f}" for intensity in peak_intensities],
                'Prominence': [f"{prom:.4f}" for prom in filtered_prominences]
            })
            st.table(peak_table)
        else:
            st.write("ãƒ”ãƒ¼ã‚¯ãŒæ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ")
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ã”ã¨ã®æç”»ã¨AIè§£æ
    for result in peak_results:
        render_peak_analysis_with_ai(result, spectrum_type, llm_connector, user_hint, llm_ready)

def render_peak_analysis_with_ai(result, spectrum_type, llm_connector, user_hint, llm_ready):
    """å€‹åˆ¥ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ”ãƒ¼ã‚¯è§£æçµæœã‚’æç”»ã—ã¦AIè§£æã‚’å®Ÿè¡Œ"""
    file_key = result['file_name']

    # åˆæœŸåŒ–
    if f"{file_key}_excluded_peaks" not in st.session_state:
        st.session_state[f"{file_key}_excluded_peaks"] = set()
    if f"{file_key}_manual_peaks" not in st.session_state:
        st.session_state[f"{file_key}_manual_peaks"] = []

    # ãƒ—ãƒ­ãƒƒãƒˆæç”»
    render_interactive_plot(result, file_key, spectrum_type)
    
    # AIè§£æã‚»ã‚¯ã‚·ãƒ§ãƒ³
    render_ai_analysis_section(result, file_key, spectrum_type, llm_connector, user_hint, llm_ready)


def render_interactive_plot(result, file_key, spectrum_type):
    """ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ—ãƒ­ãƒƒãƒˆã‚’æç”»ï¼ˆpeak_analysis_web.pyã¨åŒã˜æ–¹å¼ï¼‰"""
    st.subheader(f"ğŸ“Š {file_key} - {spectrum_type}")
    
    # ---- æ‰‹å‹•åˆ¶å¾¡UIï¼ˆpeak_analysis_web.pyã‹ã‚‰ç§»æ¤ï¼‰ ----
    col1, col2 = st.columns(2)
    
    with col1:
        st.write("**ğŸ”¹ ãƒ”ãƒ¼ã‚¯æ‰‹å‹•è¿½åŠ **")
        add_wavenum = st.number_input(
            "è¿½åŠ ã™ã‚‹æ³¢æ•° (cmâ»Â¹):",
            min_value=float(result['wavenum'].min()),
            max_value=float(result['wavenum'].max()),
            value=float(result['wavenum'][len(result['wavenum'])//2]),
            step=1.0,
            key=f"add_wavenum_{file_key}"
        )
        
        if st.button(f"æ³¢æ•° {add_wavenum:.1f} ã®ãƒ”ãƒ¼ã‚¯ã‚’è¿½åŠ ", key=f"add_peak_{file_key}"):
            # é‡è¤‡ãƒã‚§ãƒƒã‚¯ï¼ˆÂ±2 cmâ»Â¹ä»¥å†…ï¼‰
            is_duplicate = any(abs(existing_wn - add_wavenum) < 2.0 
                             for existing_wn in st.session_state[f"{file_key}_manual_peaks"])
            
            if not is_duplicate:
                st.session_state[f"{file_key}_manual_peaks"].append(add_wavenum)
                st.success(f"æ³¢æ•° {add_wavenum:.1f} cmâ»Â¹ ã«ãƒ”ãƒ¼ã‚¯ã‚’è¿½åŠ ã—ã¾ã—ãŸ")
                st.rerun()
            else:
                st.warning("è¿‘æ¥ã™ã‚‹ä½ç½®ã«ã™ã§ã«ãƒ”ãƒ¼ã‚¯ãŒå­˜åœ¨ã—ã¾ã™")
    
    with col2:
        st.write("**ğŸ”¸ æ¤œå‡ºãƒ”ãƒ¼ã‚¯é™¤å¤–**")
        if len(result['detected_peaks']) > 0:
            # æ¤œå‡ºãƒ”ãƒ¼ã‚¯ã®é¸æŠè‚¢ã‚’ä½œæˆ
            detected_options = []
            for i, idx in enumerate(result['detected_peaks']):
                wn = result['wavenum'][idx]
                intensity = result['spectrum'][idx]
                status = "é™¤å¤–æ¸ˆã¿" if idx in st.session_state[f"{file_key}_excluded_peaks"] else "æœ‰åŠ¹"
                detected_options.append(f"ãƒ”ãƒ¼ã‚¯{i+1}: {wn:.1f} cmâ»Â¹ ({intensity:.3f}) - {status}")
            
            selected_peak = st.selectbox(
                "é™¤å¤–/å¾©æ´»ã•ã›ã‚‹ãƒ”ãƒ¼ã‚¯ã‚’é¸æŠ:",
                options=range(len(detected_options)),
                format_func=lambda x: detected_options[x],
                key=f"select_peak_{file_key}"
            )
            
            peak_idx = result['detected_peaks'][selected_peak]
            is_excluded = peak_idx in st.session_state[f"{file_key}_excluded_peaks"]
            
            if is_excluded:
                if st.button(f"ãƒ”ãƒ¼ã‚¯{selected_peak+1}ã‚’å¾©æ´»", key=f"restore_peak_{file_key}"):
                    st.session_state[f"{file_key}_excluded_peaks"].remove(peak_idx)
                    st.success(f"ãƒ”ãƒ¼ã‚¯{selected_peak+1}ã‚’å¾©æ´»ã•ã›ã¾ã—ãŸ")
                    st.rerun()
            else:
                if st.button(f"ãƒ”ãƒ¼ã‚¯{selected_peak+1}ã‚’é™¤å¤–", key=f"exclude_peak_{file_key}"):
                    st.session_state[f"{file_key}_excluded_peaks"].add(peak_idx)
                    st.success(f"ãƒ”ãƒ¼ã‚¯{selected_peak+1}ã‚’é™¤å¤–ã—ã¾ã—ãŸ")
                    st.rerun()
        else:
            st.info("æ¤œå‡ºã•ã‚ŒãŸãƒ”ãƒ¼ã‚¯ãŒã‚ã‚Šã¾ã›ã‚“")

    # ---- æ‰‹å‹•è¿½åŠ ãƒ”ãƒ¼ã‚¯ç®¡ç†ãƒ†ãƒ¼ãƒ–ãƒ« ----
    if st.session_state[f"{file_key}_manual_peaks"]:
        st.write("**ğŸ“ æ‰‹å‹•è¿½åŠ ãƒ”ãƒ¼ã‚¯ä¸€è¦§**")
        manual_peaks = st.session_state[f"{file_key}_manual_peaks"]
        
        # ãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆ
        manual_data = []
        for i, wn in enumerate(manual_peaks):
            idx = np.argmin(np.abs(result['wavenum'] - wn))
            intensity = result['spectrum'][idx]
            manual_data.append({
                'ç•ªå·': i + 1,
                'æ³¢æ•° (cmâ»Â¹)': f"{wn:.1f}",
                'å¼·åº¦': f"{intensity:.3f}"
            })
        
        manual_df = pd.DataFrame(manual_data)
        st.dataframe(manual_df, use_container_width=True, key=f"manual_peaks_table_{file_key}")
        
        # å‰Šé™¤é¸æŠ
        if len(manual_peaks) > 0:
            col_del1, col_del2 = st.columns([3, 1])
            with col_del1:
                delete_idx = st.selectbox(
                    "å‰Šé™¤ã™ã‚‹æ‰‹å‹•ãƒ”ãƒ¼ã‚¯ã‚’é¸æŠ:",
                    options=range(len(manual_peaks)),
                    format_func=lambda x: f"ãƒ”ãƒ¼ã‚¯{x+1}: {manual_peaks[x]:.1f} cmâ»Â¹",
                    key=f"delete_manual_{file_key}"
                )
            with col_del2:
                if st.button("å‰Šé™¤", key=f"delete_manual_btn_{file_key}"):
                    removed_wn = st.session_state[f"{file_key}_manual_peaks"].pop(delete_idx)
                    st.success(f"æ³¢æ•° {removed_wn:.1f} cmâ»Â¹ ã®ãƒ”ãƒ¼ã‚¯ã‚’å‰Šé™¤ã—ã¾ã—ãŸ")
                    st.rerun()

    # ---- ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°æ¸ˆã¿ãƒ”ãƒ¼ã‚¯é…åˆ—ï¼ˆpeak_analysis_web.pyã¨åŒã˜ï¼‰ ----
    filtered_peaks = [
        i for i in result["detected_peaks"]
        if i not in st.session_state[f"{file_key}_excluded_peaks"]
    ]
    filtered_prominences = [
        prom for i, prom in zip(result["detected_peaks"], result["detected_prominences"])
        if i not in st.session_state[f"{file_key}_excluded_peaks"]
    ]
    
    # ---- é™çš„ãƒ—ãƒ­ãƒƒãƒˆæç”»ï¼ˆpeak_analysis_web.pyã‹ã‚‰å®Œå…¨ç§»æ¤ï¼‰ ----
    fig = make_subplots(
        rows=3, cols=1,
        shared_xaxes=True,
        vertical_spacing=0.07,
        row_heights=[0.4, 0.3, 0.3]
    )

    # 1æ®µç›®ï¼šãƒ¡ã‚¤ãƒ³ã‚¹ãƒšã‚¯ãƒˆãƒ«
    fig.add_trace(
        go.Scatter(
            x=result['wavenum'],
            y=result['spectrum'],
            mode='lines',
            name=spectrum_type,
            line=dict(color='blue', width=2)
        ),
        row=1, col=1
    )

    # è‡ªå‹•æ¤œå‡ºãƒ”ãƒ¼ã‚¯ï¼ˆæœ‰åŠ¹ãªã‚‚ã®ã®ã¿ï¼‰
    if len(filtered_peaks) > 0:
        fig.add_trace(
            go.Scatter(
                x=result['wavenum'][filtered_peaks],
                y=result['spectrum'][filtered_peaks],
                mode='markers',
                name='æ¤œå‡ºãƒ”ãƒ¼ã‚¯ï¼ˆæœ‰åŠ¹ï¼‰',
                marker=dict(color='red', size=8, symbol='circle')
            ),
            row=1, col=1
        )

    # é™¤å¤–ã•ã‚ŒãŸãƒ”ãƒ¼ã‚¯
    excluded_peaks = list(st.session_state[f"{file_key}_excluded_peaks"])
    if len(excluded_peaks) > 0:
        fig.add_trace(
            go.Scatter(
                x=result['wavenum'][excluded_peaks],
                y=result['spectrum'][excluded_peaks],
                mode='markers',
                name='é™¤å¤–ãƒ”ãƒ¼ã‚¯',
                marker=dict(color='gray', size=8, symbol='x')
            ),
            row=1, col=1
        )

    # æ‰‹å‹•ãƒ”ãƒ¼ã‚¯ï¼ˆpeak_analysis_web.pyã¨åŒã˜å‡¦ç†ï¼‰
    for wn in st.session_state[f"{file_key}_manual_peaks"]:
        idx = np.argmin(np.abs(result['wavenum'] - wn))
        intensity = result['spectrum'][idx]
        fig.add_trace(
            go.Scatter(
                x=[wn],
                y=[intensity],
                mode='markers+text',
                marker=dict(color='green', size=10, symbol='star'),
                text=["æ‰‹å‹•"],
                textposition='top center',
                name="æ‰‹å‹•ãƒ”ãƒ¼ã‚¯",
                showlegend=False
            ),
            row=1, col=1
        )

    # 2æ®µç›®ï¼š2æ¬¡å¾®åˆ†
    fig.add_trace(
        go.Scatter(
            x=result['wavenum'],
            y=result['second_derivative'],
            mode='lines',
            name='2æ¬¡å¾®åˆ†',
            line=dict(color='purple', width=1)
        ),
        row=2, col=1
    )
    fig.add_hline(y=0, line_dash="dash", line_color="black", opacity=0.5, row=2, col=1)

    # 3æ®µç›®ï¼šProminenceãƒ—ãƒ­ãƒƒãƒˆ
    fig.add_trace(
        go.Scatter(
            x=result['wavenum'][result['all_peaks']],
            y=result['all_prominences'],
            mode='markers',
            name='å…¨ãƒ”ãƒ¼ã‚¯ã®å“ç«‹åº¦',
            marker=dict(color='orange', size=4)
        ),
        row=3, col=1
    )
    if len(filtered_peaks) > 0:
        fig.add_trace(
            go.Scatter(
                x=result['wavenum'][filtered_peaks],
                y=filtered_prominences,
                mode='markers',
                name='æœ‰åŠ¹ãªå“ç«‹åº¦',
                marker=dict(color='red', size=7, symbol='circle')
            ),
            row=3, col=1
        )

    fig.update_layout(height=800, margin=dict(t=80, b=150))
    
    for r in [1, 2, 3]:
        fig.update_xaxes(
            showticklabels=True,
            title_text="æ³¢æ•° (cmâ»Â¹)" if r == 3 else "",
            row=r, col=1,
            automargin=True
        )
    
    fig.update_yaxes(title_text="Intensity (a.u.)", row=1, col=1)
    fig.update_yaxes(title_text="2æ¬¡å¾®åˆ†", row=2, col=1)
    fig.update_yaxes(title_text="Prominence", row=3, col=1)
    
    # PDFãƒ¬ãƒãƒ¼ãƒˆç”¨ã«Plotlyã‚°ãƒ©ãƒ•ã‚’ä¿å­˜
    st.session_state[f"{file_key}_plotly_figure"] = fig
    
    # å…ƒã®ã‚¹ãƒšã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜ï¼ˆPDFãƒ¬ãƒãƒ¼ãƒˆç”¨ï¼‰
    save_original_spectrum_data_to_session(result, file_key)
    
    # ã€ä¿®æ­£ã€‘ä¸€æ„ã®ã‚­ãƒ¼ã‚’è¿½åŠ ã—ã¦ã‚°ãƒ©ãƒ•è¡¨ç¤º
    st.plotly_chart(fig, use_container_width=True, key=f"plotly_chart_{file_key}")

def render_ai_analysis_section(result, file_key, spectrum_type, llm_connector, user_hint, llm_ready):
    """AIè§£æã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’æç”»"""
    st.markdown("---")
    st.subheader(f"AIè§£æ - {file_key}")
    
    # æœ€çµ‚çš„ãªãƒ”ãƒ¼ã‚¯æƒ…å ±ã‚’åé›†
    final_peak_data = []
    
    # æœ‰åŠ¹ãªè‡ªå‹•æ¤œå‡ºãƒ”ãƒ¼ã‚¯
    filtered_peaks = [
        i for i in result['detected_peaks']
        if i not in st.session_state[f"{file_key}_excluded_peaks"]
    ]
    filtered_prominences = [
        prom for i, prom in zip(result['detected_peaks'], result['detected_prominences'])
        if i not in st.session_state[f"{file_key}_excluded_peaks"]
    ]
    
    for idx, prom in zip(filtered_peaks, filtered_prominences):
        final_peak_data.append({
            'wavenumber': result['wavenum'][idx],
            'intensity': result['spectrum'][idx],
            'prominence': prom,
            'type': 'auto'
        })
    
    # æ‰‹å‹•è¿½åŠ ãƒ”ãƒ¼ã‚¯
    for x, y in st.session_state[f"{file_key}_manual_peaks"]:
        idx = np.argmin(np.abs(result['wavenum'] - x))
        try:
            # scipyè­¦å‘Šã‚’æŠ‘åˆ¶ã—ã¦prominenceè¨ˆç®—
            with warnings.catch_warnings():
                warnings.simplefilter("ignore")
                prom_values = peak_prominences(-result['second_derivative'], [idx])
                prom = prom_values[0][0] if len(prom_values[0]) > 0 else 0.0
                # prominenceå€¤ãŒ0ã¾ãŸã¯è² ã®å ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                if prom <= 0:
                    # è¿‘å‚ã®æœ€å¤§å€¤ã‚’ä½¿ç”¨ã—ã¦prominenceã‚’æ¨å®š
                    window_start = max(0, idx - 5)
                    window_end = min(len(result['second_derivative']), idx + 6)
                    local_values = -result['second_derivative'][window_start:window_end]
                    if len(local_values) > 0:
                        prom = max(0.001, np.max(local_values) - np.min(local_values))
                    else:
                        prom = 0.001  # æœ€å°å€¤ã‚’è¨­å®š
        except Exception as e:
            prom = 0.001  # ã‚¨ãƒ©ãƒ¼æ™‚ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å€¤
        
        final_peak_data.append({
            'wavenumber': x,
            'intensity': y,
            'prominence': prom,
            'type': 'manual'
        })
    
    if final_peak_data:
        st.write(f"**æœ€çµ‚ç¢ºå®šãƒ”ãƒ¼ã‚¯æ•°: {len(final_peak_data)}**")
        
        # ãƒ”ãƒ¼ã‚¯è¡¨ç¤º
        peak_summary_df = pd.DataFrame([
            {
                'ãƒ”ãƒ¼ã‚¯ç•ªå·': i+1,
                'æ³¢æ•° (cmâ»Â¹)': f"{peak['wavenumber']:.1f}",
                'å¼·åº¦': f"{peak['intensity']:.3f}",
                'Prominence': f"{peak['prominence']:.3f}",
                'ã‚¿ã‚¤ãƒ—': 'è‡ªå‹•æ¤œå‡º' if peak['type'] == 'auto' else 'æ‰‹å‹•è¿½åŠ '
            }
            for i, peak in enumerate(final_peak_data)
        ])
        st.table(peak_summary_df)
        
        # AIè§£æå®Ÿè¡Œãƒœã‚¿ãƒ³
        ai_button_disabled = not (llm_ready and final_peak_data)
        if not llm_ready:
            st.warning("OpenAI APIãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚AIè§£æã‚’å®Ÿè¡Œã™ã‚‹ã«ã¯ã€æœ‰åŠ¹ãªAPIã‚­ãƒ¼ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
        
        if st.button(f"AIè§£æã‚’å®Ÿè¡Œ - {file_key}", key=f"ai_analysis_{file_key}", disabled=ai_button_disabled):
            perform_ai_analysis(file_key, final_peak_data, user_hint, llm_connector, peak_summary_df)
        
        # éå»ã®è§£æçµæœè¡¨ç¤º
        if f"{file_key}_ai_analysis" in st.session_state:
            with st.expander("ğŸ“œ éå»ã®è§£æçµæœã‚’è¡¨ç¤º"):
                past_analysis = st.session_state[f"{file_key}_ai_analysis"]
                st.write(f"**è§£ææ—¥æ™‚:** {past_analysis['timestamp']}")
                st.write(f"**ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«:** {past_analysis['model']}")
                st.markdown("**è§£æçµæœ:**")
                st.markdown(past_analysis['analysis'])
            
            
            # è³ªå•å¿œç­”ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¡¨ç¤º
            if llm_ready:
                render_qa_section(
                    file_key=file_key,
                    analysis_context=st.session_state[f"{file_key}_ai_analysis"]['analysis_context'],
                    llm_connector=llm_connector
                )

            # ãƒ¬ãƒãƒ¼ãƒˆãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚»ã‚¯ã‚·ãƒ§ãƒ³ï¼ˆè¿½åŠ è³ªå•ã®ä¸‹ï¼‰
            st.markdown("---")
            st.subheader("ãƒ¬ãƒãƒ¼ãƒˆãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰")
            
            # éå»ã®è§£æçµæœãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
            if f"{file_key}_ai_analysis" in st.session_state:
                # è§£æçµæœã‹ã‚‰å¿…è¦ãªãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
                past_analysis = st.session_state[f"{file_key}_ai_analysis"]
                saved_peak_data = past_analysis.get('peak_data', [])
                saved_peak_summary_df = past_analysis.get('peak_summary_df', pd.DataFrame())
                saved_relevant_docs = past_analysis.get('relevant_docs', [])
                saved_user_hint = past_analysis.get('user_hint', '')
                
                # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æƒ…å ±ã‚’å–å¾—
                database_info = None
                database_files = []
                if hasattr(st.session_state, 'rag_system') and st.session_state.rag_system:
                    database_info = st.session_state.rag_system.get_database_info()
                    if database_info.get('source_files'):
                        database_files = database_info['source_files']
                
                col1, col2 = st.columns(2)
                
                # ãƒ†ã‚­ã‚¹ãƒˆãƒ¬ãƒãƒ¼ãƒˆãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
                with col1:
                    # ãƒ†ã‚­ã‚¹ãƒˆãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
                    analysis_report = f"""ãƒ©ãƒãƒ³ã‚¹ãƒšã‚¯ãƒˆãƒ«è§£æãƒ¬ãƒãƒ¼ãƒˆ
        ãƒ•ã‚¡ã‚¤ãƒ«å: {file_key}
        è§£ææ—¥æ™‚: {past_analysis['timestamp']}
        ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«: {past_analysis['model']}
        
        === ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æƒ…å ± ===
        """
                    if database_info:
                        analysis_report += f"""ä½œæˆæ—¥æ™‚: {database_info.get('created_at', 'N/A')}
        ä½œæˆè€…: {database_info.get('created_by', 'N/A')}
        ç·æ–‡çŒ®æ•°: {database_info.get('n_docs', 0)}
        ç·ãƒãƒ£ãƒ³ã‚¯æ•°: {database_info.get('n_chunks', 0)}
        ä½¿ç”¨ãƒ•ã‚¡ã‚¤ãƒ«: {', '.join(database_files) if database_files else 'ãªã—'}
        """
                    else:
                        analysis_report += "ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã¯ä½¿ç”¨ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚\n"
                    
                    if saved_user_hint:
                        analysis_report += f"\n=== AIã¸ã®è£œè¶³ãƒ’ãƒ³ãƒˆ ===\n{saved_user_hint}\n"
                    
                    analysis_report += f"""
        === æ¤œå‡ºãƒ”ãƒ¼ã‚¯æƒ…å ± ===
        {saved_peak_summary_df.to_string(index=False)}
        
        === AIè§£æçµæœ ===
        {past_analysis['analysis']}
        
        === è¿½åŠ è³ªå•å±¥æ­´ ===
        """
                    qa_history_key = f"{file_key}_qa_history"  # ã“ã®è¡Œã‚’è¿½åŠ 
                    for i, qa in enumerate(st.session_state[qa_history_key], 1):
                        analysis_report += f"è³ªå•{i}: {qa['question']}\nå›ç­”{i}: {qa['answer']}\nè³ªå•æ—¥æ™‚: {qa['timestamp']}\n\n"
                    
                    analysis_report += "=== å‚ç…§æ–‡çŒ® ===\n"
                    for i, doc in enumerate(saved_relevant_docs, 1):
                        analysis_report += f"{i}. {doc['metadata']['filename']}ï¼ˆé¡ä¼¼åº¦: {doc['similarity_score']:.3f}ï¼‰\n"
                    
                    st.download_button(
                        label="ãƒ†ã‚­ã‚¹ãƒˆãƒ¬ãƒãƒ¼ãƒˆã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                        data=analysis_report,
                        file_name=f"raman_analysis_report_{file_key}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt",
                        mime="text/plain",
                        key=f"download_text_report_{file_key}"
                    )
                
                # PDFãƒ¬ãƒãƒ¼ãƒˆãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
                with col2:
                    if PDF_GENERATION_AVAILABLE:
                        if st.button(f"ğŸ“Š PDFãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ", key=f"generate_comprehensive_pdf_{file_key}"):
                            try:
                                with st.spinner("PDFãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆä¸­..."):
                                    # PDFãƒ¬ãƒãƒ¼ãƒˆã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ãƒ¼ã‚’åˆæœŸåŒ–
                                    pdf_generator = RamanPDFReportGenerator()
                                    
                                    # ç¾åœ¨è¡¨ç¤ºã•ã‚Œã¦ã„ã‚‹Plotlyã‚°ãƒ©ãƒ•ã‚’å–å¾—
                                    plotly_figure = st.session_state.get(f"{file_key}_plotly_figure", None)
                                    
                                    # Q&Aå±¥æ­´ã‚’å–å¾—
                                    qa_history_key = f"{file_key}_qa_history"
                                    qa_history = st.session_state[qa_history_key]
                                    
                                    # å…ƒã®ã‚¹ãƒšã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
                                    original_spectrum_data = st.session_state.get(f"{file_key}_original_spectrum_data", None)
                                    
                                    # PDFã‚’ç”Ÿæˆ
                                    pdf_bytes = pdf_generator.generate_pdf_report(
                                        file_key=file_key,
                                        peak_data=saved_peak_data,
                                        analysis_result=past_analysis['analysis'],
                                        peak_summary_df=saved_peak_summary_df,
                                        plotly_figure=plotly_figure,
                                        relevant_docs=saved_relevant_docs,
                                        user_hint=saved_user_hint,
                                        qa_history=qa_history,
                                        database_info=database_info,
                                        database_files=database_files,
                                        original_spectrum_data=original_spectrum_data,
                                    )
                                    
                                    # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
                                    try:
                                        pdf_generator.cleanup_temp_files()
                                    except:
                                        pass
                                    
                                    # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒœã‚¿ãƒ³ã‚’è¡¨ç¤º
                                    st.download_button(
                                        label="PDFãƒ¬ãƒãƒ¼ãƒˆã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                                        data=pdf_bytes,
                                        file_name=f"raman_comprehensive_report_{file_key}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pdf",
                                        mime="application/pdf",
                                        key=f"download_comprehensive_pdf_report_{file_key}"
                                    )
                                    
                                    st.success("âœ… PDFãƒ¬ãƒãƒ¼ãƒˆãŒæ­£å¸¸ã«ç”Ÿæˆã•ã‚Œã¾ã—ãŸï¼")
                                    
                            except Exception as e:
                                st.error(f"PDFãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã‚¨ãƒ©ãƒ¼: {str(e)}")
                                # st.info("PDFãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã«å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªï¼ˆreportlab, Pillowï¼‰ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
                    else:
                        st.info("PDFãƒ¬ãƒãƒ¼ãƒˆæ©Ÿèƒ½ã¯åˆ©ç”¨ã§ãã¾ã›ã‚“ï¼ˆå¿…è¦ãƒ©ã‚¤ãƒ–ãƒ©ãƒªæœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼‰")
            else:
                st.info("AIè§£æçµæœãŒãªã„ãŸã‚ã€ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆã§ãã¾ã›ã‚“ã€‚å…ˆã«AIè§£æã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
            
    else:
        st.info("ç¢ºå®šã•ã‚ŒãŸãƒ”ãƒ¼ã‚¯ãŒã‚ã‚Šã¾ã›ã‚“ã€‚ãƒ”ãƒ¼ã‚¯æ¤œå‡ºã‚’å®Ÿè¡Œã™ã‚‹ã‹ã€æ‰‹å‹•ã§ãƒ”ãƒ¼ã‚¯ã‚’è¿½åŠ ã—ã¦ãã ã•ã„ã€‚")

def perform_ai_analysis(file_key, final_peak_data, user_hint, llm_connector, peak_summary_df):
    """AIè§£æã‚’å®Ÿè¡Œï¼ˆPDFæ©Ÿèƒ½ä»˜ãï¼‰"""
    with st.spinner("AIè§£æä¸­ã§ã™ã€‚ã—ã°ã‚‰ããŠå¾…ã¡ãã ã•ã„..."):
        analysis_report = None
        start_time = time.time()

        try:
            analyzer = RamanSpectrumAnalyzer()

            # é–¢é€£æ–‡çŒ®ã‚’æ¤œç´¢
            search_terms = ' '.join([f"{p['wavenumber']:.0f}cm-1" for p in final_peak_data[:5]])
            search_query = f"ãƒ©ãƒãƒ³ã‚¹ãƒšã‚¯ãƒˆãƒ­ã‚¹ã‚³ãƒ”ãƒ¼ ãƒ”ãƒ¼ã‚¯ {search_terms}"
            relevant_docs = st.session_state.rag_system.search_relevant_documents(search_query, top_k=5)

            # AIã¸ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç”Ÿæˆ
            analysis_prompt = analyzer.generate_analysis_prompt(
                peak_data=final_peak_data,
                relevant_docs=relevant_docs,
                user_hint=user_hint
            )
            
            # OpenAI APIã§è§£æã‚’å®Ÿè¡Œ
            st.success("AIã«ã‚ˆã‚‹å›ç­”")
            full_response = llm_connector.generate_analysis(analysis_prompt)

            # å‡¦ç†æ™‚é–“ã®è¡¨ç¤º
            elapsed = time.time() - start_time
            st.info(f"è§£æã«ã‹ã‹ã£ãŸæ™‚é–“: {elapsed:.2f} ç§’")

            # è§£æçµæœã‚’ã‚»ãƒƒã‚·ãƒ§ãƒ³ã«ä¿å­˜
            model_info = f"OpenAI ({llm_connector.selected_model})"
            st.session_state[f"{file_key}_ai_analysis"] = {
                'analysis': full_response,
                'timestamp': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                'model': model_info,
                'analysis_context': full_response,
                'peak_data': final_peak_data,
                'peak_summary_df': peak_summary_df,
                'relevant_docs': relevant_docs,
                'user_hint': user_hint
            }

        except Exception as e:
            st.error(f"AIè§£æä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
            st.info("OpenAI APIã®æ¥ç¶šã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚æœ‰åŠ¹ãªAPIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
