# -*- coding: utf-8 -*-
"""
Created on Wed Jun 11 15:56:04 2025

@author: hiroy
"""

import numpy as np
import pandas as pd
import streamlit as st

import scipy.signal as signal
import matplotlib.pyplot as plt

from scipy.sparse.linalg import spsolve
from scipy.sparse import csc_matrix, eye, diags
from sklearn.ensemble import RandomForestClassifier
from sklearn.decomposition import NMF, PCA
from sklearn.preprocessing import StandardScaler
import seaborn as sns

# Set matplotlib style
plt.style.use('default')
sns.set_palette("husl")

def create_features_labels(spectra, window_size=10):
    # ç‰¹å¾´é‡ã¨ãƒ©ãƒ™ãƒ«ã®é…åˆ—ã‚’åˆæœŸåŒ–
    X = []
    y = []
    # ã‚¹ãƒšã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿ã®é•·ã•
    n_points = len(spectra)
    # äººæ‰‹ã«ã‚ˆã‚‹ãƒ”ãƒ¼ã‚¯ãƒ©ãƒ™ãƒ«ã€ã¾ãŸã¯è‡ªå‹•ç”Ÿæˆã‚³ãƒ¼ãƒ‰ã‚’ã“ã“ã«é…ç½®
    peak_labels = np.zeros(n_points)

    # ç‰¹å¾´é‡ã¨ãƒ©ãƒ™ãƒ«ã®æŠ½å‡º
    for i in range(window_size, n_points - window_size):
        # å‰å¾Œã®çª“ã‚µã‚¤ã‚ºã®ãƒ‡ãƒ¼ã‚¿ã‚’ç‰¹å¾´é‡ã¨ã—ã¦ä½¿ç”¨
        features = spectra[i-window_size:i+window_size+1]
        X.append(features)
        y.append(peak_labels[i])

    return np.array(X), np.array(y)

def find_index(rs_array,  rs_focused):
    '''
    Convert the index of the proximate wavenumber by finding the absolute 
    minimum value of (rs_array - rs_focused)
    
    input
        rs_array: Raman wavenumber
        rs_focused: Index
    output
        index
    '''

    diff = [abs(element - rs_focused) for element in rs_array]
    index = np.argmin(diff)
    return index

def WhittakerSmooth(x,w,lambda_,differences=1):
    '''
    Penalized least squares algorithm for background fitting
    
    input
        x: input data (i.e. chromatogram of spectrum)
        w: binary masks (value of the mask is zero if a point belongs to peaks and one otherwise)
        lambda_: parameter that can be adjusted by user. The larger lambda is,  the smoother the resulting background
        differences: integer indicating the order of the difference of penalties
    
    output
        the fitted background vector
    '''
    X=np.array(x, dtype=np.float64)
    m=X.size
    E=eye(m,format='csc')
    for i in range(differences):
        E=E[1:]-E[:-1] # numpy.diff() does not work with sparse matrix. This is a workaround.
    W=diags(w,0,shape=(m,m))
    A=csc_matrix(W+(lambda_*E.T*E))
    B=csc_matrix(W*X.T).toarray().flatten()
    background=spsolve(A,B)
    return np.array(background)

def airPLS(x, dssn_th, lambda_, porder, itermax):
    '''
    Adaptive iteratively reweighted penalized least squares for baseline fitting
    
    input
        x: input data (i.e. chromatogram or spectrum)
        lambda_: parameter that can be adjusted by user. The larger lambda is, the smoother the resulting background, z
        porder: adaptive iteratively reweighted penalized least squares for baseline fitting
    
    output
        the fitted background vector
    '''
    # ãƒã‚¤ãƒŠã‚¹å€¤ãŒã‚ã‚‹å ´åˆã®å‡¦ç†
    min_value = np.min(x)
    print(np.min(x))
    offset = 0
    if min_value < 0:
        offset = abs(min_value) + 1  # æœ€å°å€¤ã‚’1ã«ã™ã‚‹ãŸã‚ã®ã‚ªãƒ•ã‚»ãƒƒãƒˆ
        x = x + offset  # å…¨ä½“ã‚’ã‚·ãƒ•ãƒˆ
    print(np.min(x))
    m = x.shape[0]
    w = np.ones(m, dtype=np.float64)  # æ˜ç¤ºçš„ã«å‹ã‚’æŒ‡å®š
    x = np.asarray(x, dtype=np.float64)  # xã‚‚æ˜ç¤ºçš„ã«å‹ã‚’æŒ‡å®š
    
    for i in range(1, itermax + 1):
        z = WhittakerSmooth(x, w, lambda_, porder)
        d = x - z
        dssn = np.abs(d[d < 0].sum())
        
        # dssn ãŒã‚¼ãƒ­ã¾ãŸã¯éå¸¸ã«å°ã•ã„å ´åˆã‚’å›é¿
        if dssn < 1e-10:
            dssn = 1e-10
        
        # åæŸåˆ¤å®š
        if (dssn < dssn_th * (np.abs(x)).sum()) or (i == itermax):
            if i == itermax:
                print('WARNING: max iteration reached!')
            break
        
        # é‡ã¿ã®æ›´æ–°
        w[d >= 0] = 0  # d > 0 ã¯ãƒ”ãƒ¼ã‚¯ã®ä¸€éƒ¨ã¨ã—ã¦é‡ã¿ã‚’ç„¡è¦–
        w[d < 0] = np.exp(i * np.abs(d[d < 0]) / dssn)
        
        # å¢ƒç•Œæ¡ä»¶ã®èª¿æ•´
        if d[d < 0].size > 0:
            w[0] = np.exp(i * np.abs(d[d < 0]).max() / dssn)
        else:
            w[0] = 1.0  # é©åˆ‡ãªåˆæœŸå€¤
        
        w[-1] = w[0]

    return z

def savitzky_golay(y, window_size, order, deriv=0, rate=1):
    r"""Smooth (and optionally differentiate) data with a Savitzky-Golay filter.
    The Savitzky-Golay filter removes high frequency noise from data.
    It has the advantage of preserving the original shape and
    features of the signal better than other types of filtering
    approaches, such as moving averages techniques.
    Parameters
    ----------
    y : array_like, shape (N,)
        the values of the time history of the signal.
    window_size : int
        the length of the window. Must be an odd integer number.
    order : int
        the order of the polynomial used in the filtering.
        Must be less then `window_size` - 1.
    deriv: int
        the order of the derivative to compute (default = 0 means only smoothing)
    Returns
    -------
    ys : ndarray, shape (N)
        the smoothed signal (or it's n-th derivative).
    Notes
    -----
    The Savitzky-Golay is a type of low-pass filter, particularly
    suited for smoothing noisy data. The main idea behind this
    approach is to make for each point a least-square fit with a
    polynomial of high order over a odd-sized window centered at
    the point.
    Examples
    --------
    t = np.linspace(-4, 4, 500)
    y = np.exp( -t**2 ) + np.random.normal(0, 0.05, t.shape)
    ysg = savitzky_golay(y, window_size=31, order=4)
    import matplotlib.pyplot as plt
    plt.plot(t, y, label='Noisy signal')
    plt.plot(t, np.exp(-t**2), 'k', lw=1.5, label='Original signal')
    plt.plot(t, ysg, 'r', label='Filtered signal')
    plt.legend()
    plt.show()
    References
    ----------
    .. [1] A. Savitzky, M. J. E. Golay, Smoothing and Differentiation of
       Data by Simplified Least Squares Procedures. Analytical
       Chemistry, 1964, 36 (8), pp 1627-1639.
    .. [2] Numerical Recipes 3rd Edition: The Art of Scientific Computing
       W.H. Press, S.A. Teukolsky, W.T. Vetterling, B.P. Flannery
       Cambridge University Press ISBN-13: 9780521880688
    """
    import numpy as np
    from math import factorial

    try:
        window_size = np.abs(int(window_size))
        order = np.abs(int(order))
    except ValueError as msg:
        raise ValueError("window_size and order have to be of type int")
    if window_size % 2 != 1 or window_size < 1:
        raise TypeError("window_size size must be a positive odd number")
    if window_size < order + 2:
        raise TypeError("window_size is too small for the polynomials order")
    
    order_range = range(order+1)
    half_window = (window_size -1) // 2
    # precompute coefficients
    b = np.asmatrix([[k**i for i in order_range] for k in range(-half_window, half_window+1)])
    m = np.linalg.pinv(b).A[deriv] * rate**deriv * factorial(deriv)
    # pad the signal at the extremes with
    # values taken from the signal itself
    firstvals = y[0] - np.abs( y[1:half_window+1][::-1] - y[0] )
    lastvals = y[-1] + np.abs(y[-half_window-1:-1][::-1] - y[-1])
    y = np.concatenate((firstvals, y, lastvals))
    return np.convolve( m[::-1], y, mode='valid')

def find_peak_width(spectra, first_dev, peak_position, window_size=20):
    """
    Find the peak start/end close the peak position
    Parameters:
    spectra (ndarray): Original spectrum 
    first_dev (ndarray): First derivative of the spectrum 
    peak_position (int): Peak index 
    window_size (int): Window size to find the start/end of the peak 

    Returns:
    local_start_idx/local_end_idx: Start and end of the peaks 
    """

    start_idx = max(peak_position - window_size, 0)
    end_idx   = min(peak_position + window_size, len(first_dev) - 1)
    
    local_start_idx = np.argmax(first_dev[start_idx:end_idx+1]) + start_idx
    local_end_idx   = np.argmin(first_dev[start_idx:end_idx+1]) + start_idx
        
    return local_start_idx, local_end_idx

def find_peak_area(spectra, local_start_idx, local_end_idx):
    """
    Calculate the area of the peaks 

    Parameters:
    spectra (ndarray): Original spectrum 
    local_start_idx (int): Output of the find_peak_width
    local_end_idx (int): Output of the find_peak_width
    
    Returns:
    peak_area (float): Area of the peaks 
    """    
    
    peak_area = np.trapz(spectra[local_start_idx:local_end_idx+1], dx=1)
    
    return peak_area

def detect_file_type(data):
    """
    Determine the structure of the input data.
    """
    try:
        if data.columns[0].split(':')[0] == "# Laser Wavelength":
            return "ramaneye_new"
        elif data.columns[0] == "WaveNumber":
            return "ramaneye_old"
        elif data.columns[0] == "Pixels":
            return "eagle"
        elif data.columns[0] == "ENLIGHTEN Version":
            return "wasatch"
        return "unknown"
    except:
        return "unknown"

def read_csv_file(uploaded_file, file_extension):
    """
    Read a CSV or TXT file into a DataFrame based on file extension.
    """
    try:
        uploaded_file.seek(0)
        if file_extension == "csv":
            data = pd.read_csv(uploaded_file, sep=',', header=0, index_col=None, on_bad_lines='skip')
        else:
            data = pd.read_csv(uploaded_file, sep='\t', header=0, index_col=None, on_bad_lines='skip')
        return data
    except UnicodeDecodeError:
        uploaded_file.seek(0)
        if file_extension == "csv":
            data = pd.read_csv(uploaded_file, sep=',', encoding='shift_jis', header=0, index_col=None, on_bad_lines='skip')
        else:
            data = pd.read_csv(uploaded_file, sep='\t', encoding='shift_jis', header=0, index_col=None, on_bad_lines='skip')
        return data
    except Exception as e:
        st.error(f"Error reading file: {e}")
        return None

def remove_outliers_and_interpolate(spectrum, window_size=10, threshold_factor=3):
    """
    ã‚¹ãƒšã‚¯ãƒˆãƒ«ã‹ã‚‰ã‚¹ãƒ‘ã‚¤ã‚¯ï¼ˆå¤–ã‚Œå€¤ï¼‰ã‚’æ¤œå‡ºã—ã€è£œå®Œã™ã‚‹é–¢æ•°
    ã‚¹ãƒ‘ã‚¤ã‚¯ã¯ã€ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦å†…ã®æ¨™æº–åå·®ãŒä¸€å®šã®é–¾å€¤ã‚’è¶…ãˆã‚‹å ´åˆã«æ¤œå‡ºã•ã‚Œã‚‹
    
    input:
        spectrum: numpy array, ãƒ©ãƒãƒ³ã‚¹ãƒšã‚¯ãƒˆãƒ«
        window_size: ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã®ã‚µã‚¤ã‚ºï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯20ï¼‰
        threshold_factor: æ¨™æº–åå·®ã®é–¾å€¤ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯5å€ï¼‰
    
    output:
        cleaned_spectrum: numpy array, ã‚¹ãƒ‘ã‚¤ã‚¯ã‚’å–ã‚Šé™¤ãè£œå®Œã—ãŸã‚¹ãƒšã‚¯ãƒˆãƒ«
    """
    spectrum_len = len(spectrum)
    cleaned_spectrum = spectrum.copy()
    
    for i in range(spectrum_len):
        # ç«¯ç‚¹ã§ã¯ã€ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚µã‚¤ã‚ºãŒè¶³ã‚Šãªã„ã®ã§ã€ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚’èª¿æ•´
        left_idx = max(i - window_size, 0)
        right_idx = min(i + window_size + 1, spectrum_len)
        
        # ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦å†…ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
        window = spectrum[left_idx:right_idx]
        
        # ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦å†…ã®ä¸­å¤®å€¤ã¨æ¨™æº–åå·®ã‚’è¨ˆç®—
        window_median = np.median(window)
        window_std = np.std(window)
        
        # ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦å†…ã®å€¤ãŒæ¨™æº–åå·®ã®é–¾å€¤ã‚’è¶…ãˆã‚‹ã‚¹ãƒ‘ã‚¤ã‚¯ã‚’æ¤œå‡º
        if abs(spectrum[i] - window_median) > threshold_factor * window_std:
            # ã‚¹ãƒ‘ã‚¤ã‚¯ãŒè¦‹ã¤ã‹ã£ãŸå ´åˆã€ãã®å€¤ã‚’ä¸¡éš£ã®ä¸­å¤®å€¤ã§è£œå®Œ
            if i > 0 and i < spectrum_len - 1:  # ä¸¡éš£ã®å€¤ãŒå­˜åœ¨ã™ã‚‹å ´åˆ
                cleaned_spectrum[i] = (spectrum[i - 1] + spectrum[i + 1]) / 2
            elif i == 0:  # å·¦ç«¯ã®å ´åˆ
                cleaned_spectrum[i] = spectrum[i + 1]
            elif i == spectrum_len - 1:  # å³ç«¯ã®å ´åˆ
                cleaned_spectrum[i] = spectrum[i - 1] 
    return cleaned_spectrum

# Functions for multivariate analysis
def load_and_process_data_for_multivariate(uploaded_files, start_wavenum, end_wavenum, lambda_param, porder, savgol_wsize=5):
    """
    Load and process uploaded CSV files with support for multiple file types for multivariate analysis
    """
    grouped_data = {}
    all_baseline_spectra = []
    all_labels = []
    wavenumber = None
    dssn_th = 0.00001
    
    for uploaded_file in uploaded_files:
        try:
            file_name = uploaded_file.name
            file_extension = file_name.split('.')[-1].lower()
            
            st.write(f"Processing: {file_name}")
            
            # Read and detect file type
            data = read_csv_file(uploaded_file, file_extension)
            file_type = detect_file_type(data)
            uploaded_file.seek(0)
            
            if file_type == "unknown":
                st.error(f"{file_name}ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚¿ã‚¤ãƒ—ã‚’åˆ¤åˆ¥ã§ãã¾ã›ã‚“ã€‚")
                continue
            
            # Process each file type
            if file_type == "wasatch":
                st.write(f"ãƒ•ã‚¡ã‚¤ãƒ«ã‚¿ã‚¤ãƒ—: Wasatch ENLIGHTEN - {file_name}")
                lambda_ex = 785
                data = pd.read_csv(uploaded_file, skiprows=46)                
                pre_wavelength = np.array(data["Wavelength"].values)
                pre_wavenum = (1e7 / lambda_ex) - (1e7 / pre_wavelength)
                
                # Get number of available spectra
                number_of_rows = data.shape[1] - 3
                
                if number_of_rows > 0:
                    # Use the last available spectrum
                    number_line = number_of_rows - 1
                    if number_line == 0:
                        pre_spectrum = np.array(data["Processed"].values)
                    else:
                        pre_spectrum = np.array(data[f"Processed.{number_line}"].values)
                else:
                    pre_spectrum = np.array(data["Processed"].values)
                
            elif file_type == "ramaneye_old":
                st.write(f"ãƒ•ã‚¡ã‚¤ãƒ«ã‚¿ã‚¤ãƒ—: RamanEye Data(Old) - {file_name}")
                # number_of_rows = data.shape[1]
                
                df_transposed = data.set_index("WaveNumber").T

                # åˆ—åï¼ˆ"acrylic board" ãªã©ï¼‰ã‚’æ±ç”¨åŒ–
                df_transposed.columns = ["intensity"]
                
                # æ³¢æ•°ã‚’floatã«å¤‰æ›ã—ã€ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«è¨­å®š
                df_transposed.index = df_transposed.index.astype(float)
                df_transposed = df_transposed.sort_index()
                
                # æ³¢æ•°ã¨å¼·åº¦ã‚’NumPyé…åˆ—ã¨ã—ã¦å–å¾—
                pre_wavenum = df_transposed.index.to_numpy()
                pre_spectra = df_transposed["intensity"].to_numpy()
                
                if pre_wavenum[0] > pre_wavenum[1]:
                    # pre_wavenum ã¨ pre_spectra ã‚’åè»¢
                    pre_wavenum = pre_wavenum[::-1]
                    pre_spectra = pre_spectra[::-1]
                        
            elif file_type == "ramaneye_new":
                st.write(f"ãƒ•ã‚¡ã‚¤ãƒ«ã‚¿ã‚¤ãƒ—: RamanEye Data(New) - {file_name}")
                data = pd.read_csv(uploaded_file, skiprows=9)
                number_of_rows = data.shape[1]
                
                # Use the last available column
                number_line = number_of_rows - 2
                pre_wavenum = data["WaveNumber"]
                pre_spectrum = np.array(data.iloc[:, number_line + 1])
                
                if pre_wavenum.iloc[0] >= pre_wavenum.iloc[1]:
                    # Reverse pre_wavenum and pre_spectrum
                    pre_wavenum = pre_wavenum[::-1]
                    pre_spectrum = pre_spectrum[::-1]
                    
            elif file_type == "eagle":
                st.write(f"ãƒ•ã‚¡ã‚¤ãƒ«ã‚¿ã‚¤ãƒ—: Eagle Data - {file_name}")
                data_transposed = data.transpose()
                header = data_transposed.iloc[:3]  # æœ€åˆã®3è¡Œ
                reversed_data = data_transposed.iloc[3:].iloc[::-1]
                data_transposed = pd.concat([header, reversed_data], ignore_index=True)
                pre_wavenum = np.array(data_transposed.iloc[3:, 0])
                pre_spectra = np.array(data_transposed.iloc[3:, 1])
            
            # Convert to numpy arrays if needed
            if isinstance(pre_wavenum, pd.Series):
                pre_wavenum = pre_wavenum.values
            if isinstance(pre_spectrum, pd.Series):
                pre_spectrum = pre_spectrum.values
            
            # Find indices for wavenumber range
            start_index = find_index(pre_wavenum, start_wavenum)
            end_index = find_index(pre_wavenum, end_wavenum)

            wavenum = np.array(pre_wavenum[start_index:end_index+1])
            spectrum = np.array(pre_spectrum[start_index:end_index+1])

            # Baseline and spike removal 
            spectrum_spikerm = remove_outliers_and_interpolate(spectrum)
            
            # Apply median filter
            mveAve_spectrum = signal.medfilt(spectrum_spikerm, savgol_wsize)
            
            # Baseline correction
            baseline = airPLS(mveAve_spectrum, dssn_th, lambda_param, porder, 30)
            BSremoval_spectrum = spectrum_spikerm - baseline
            BSremoval_spectrum_pos = BSremoval_spectrum + abs(np.minimum(spectrum_spikerm, 0))  # Correct negative values
        
            # Use the baseline corrected spectrum
            corrected_spectrum = BSremoval_spectrum_pos
            
            if wavenumber is None:
                wavenumber = wavenum
            
            # Extract group name from filename
            parts = file_name.split('_')
            if len(parts) >= 2:
                group_name = parts[1]  # Use second part as group name
            else:
                group_name = file_name.split('.')[0]  # Use filename without extension
            
            if group_name not in grouped_data:
                grouped_data[group_name] = []
            
            # Store processed data
            processed_data = np.column_stack([
                wavenum,
                spectrum,
                corrected_spectrum
            ])
            grouped_data[group_name].append(processed_data)
            
            # Add to combined data
            all_baseline_spectra.append(corrected_spectrum)
            all_labels.append(group_name)
            
            st.success(f"Successfully processed {file_name} - Group: {group_name}, Data points: {len(wavenum)}")
            
        except Exception as e:
            st.error(f"Error processing file {uploaded_file.name}: {e}")
            import traceback
            st.error(f"Detailed error: {traceback.format_exc()}")
    
    return grouped_data, np.array(all_baseline_spectra), all_labels, wavenumber

def plot_spectra_matplotlib(grouped_data, title="Baseline Corrected Spectra"):
    """
    Plot spectra using matplotlib
    """
    fig, ax = plt.subplots(figsize=(12, 6))
    
    colors = ['red', 'blue', 'green', 'orange', 'purple', 'brown', 'pink', 'gray', 'olive', 'cyan']
    
    for i, (group, spectra_list) in enumerate(grouped_data.items()):
        color = colors[i % len(colors)]
        
        for j, spectrum in enumerate(spectra_list):
            label = group if j == 0 else None  # Only show legend for first spectrum of each group
            ax.plot(spectrum[:, 0], spectrum[:, 2], color=color, alpha=0.7, label=label)
    
    ax.set_title(title, fontsize=14, fontweight='bold')
    ax.set_xlabel("Wavenumber (cmâ»Â¹)", fontsize=12)
    ax.set_ylabel("Intensity", fontsize=12)
    ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
    ax.grid(True, alpha=0.3)
    plt.tight_layout()
    
    return fig

def perform_nmf_analysis(all_spectra, n_components):
    """
    Perform NMF analysis
    """
    # Ensure non-negative values for NMF
    all_spectra[all_spectra < 0] = 0
    
    # Apply NMF
    nmf_model = NMF(n_components=n_components, init='random', random_state=42, max_iter=1000)
    W = nmf_model.fit_transform(all_spectra)  # Concentration profiles
    H = nmf_model.components_  # Spectral profiles
    
    return W, H, nmf_model

def plot_nmf_components(H, wavenumber):
    """
    Plot NMF components
    """
    fig, axes = plt.subplots(len(H), 1, figsize=(12, 3 * len(H)))
    if len(H) == 1:
        axes = [axes]
    
    colors = ['red', 'blue', 'green', 'orange', 'purple']
    
    for i, (ax, spectrum) in enumerate(zip(axes, H)):
        ax.plot(wavenumber, spectrum, color=colors[i % len(colors)], linewidth=2)
        ax.set_title(f"Component {i+1}", fontsize=12, fontweight='bold')
        ax.set_xlabel("Wavenumber (cmâ»Â¹)")
        ax.set_ylabel("Intensity")
        ax.grid(True, alpha=0.3)
    
    plt.tight_layout()
    return fig

def plot_nmf_scores(W, all_labels, group_names):
    """
    Plot NMF concentration scores
    """
    if W.shape[1] >= 2:
        fig, ax = plt.subplots(figsize=(10, 8))
        
        colors = ['red', 'blue', 'green', 'orange', 'purple', 'brown', 'pink', 'gray', 'olive', 'cyan']
        
        for i, group in enumerate(group_names):
            group_indices = [j for j, label in enumerate(all_labels) if label == group]
            
            ax.scatter(W[group_indices, 0], W[group_indices, 1], 
                      label=group, color=colors[i % len(colors)], 
                      s=80, alpha=0.7, edgecolors='black', linewidth=0.5)
        
        ax.set_title("Score Plot (Component 1 vs Component 2)", fontsize=14, fontweight='bold')
        ax.set_xlabel("Component 1", fontsize=12)
        ax.set_ylabel("Component 2", fontsize=12)
        ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
        ax.grid(True, alpha=0.3)
        plt.tight_layout()
        
        return fig
    else:
        st.warning("Need at least 2 components for score plot")
        return None

def plot_contribution_ratios(contribution_ratios):
    """
    Plot component contribution ratios as bar chart
    """
    fig, ax = plt.subplots(figsize=(10, 6))
    
    components = [f"Component {i+1}" for i in range(len(contribution_ratios))]
    percentages = contribution_ratios * 100
    
    bars = ax.bar(components, percentages, color=plt.cm.Set3(np.linspace(0, 1, len(components))))
    
    # Add value labels on bars
    for bar, pct in zip(bars, percentages):
        height = bar.get_height()
        ax.text(bar.get_x() + bar.get_width()/2., height + 0.5,
                f'{pct:.1f}%', ha='center', va='bottom', fontweight='bold')
    
    ax.set_title("Component Contribution Ratios", fontsize=14, fontweight='bold')
    ax.set_xlabel("Components", fontsize=12)
    ax.set_ylabel("Contribution (%)", fontsize=12)
    ax.grid(True, alpha=0.3, axis='y')
    plt.tight_layout()
    
    return fig

def spectrum_analysis_mode():
    """
    Spectrum analysis mode (original functionality)
    """
    st.header("ğŸ“Š ãƒ©ãƒãƒ³ã‚¹ãƒšã‚¯ãƒˆãƒ«è§£æ")
    
    # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®š
    savgol_wsize         = 5    # Savitzky-Golayãƒ•ã‚£ãƒ«ã‚¿ã®ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚µã‚¤ã‚º
    savgol_order         = 3    # Savitzky-Golayãƒ•ã‚£ãƒ«ã‚¿ã®æ¬¡æ•°
    pre_start_wavenum    = 400  # æ³¢æ•°ã®é–‹å§‹
    pre_end_wavenum      = 2000 # æ³¢æ•°ã®çµ‚äº†
    Fsize                = 14   # ãƒ•ã‚©ãƒ³ãƒˆã‚µã‚¤ã‚º
    
    # æ³¢æ•°ç¯„å›²ã®è¨­å®š
    start_wavenum = st.sidebar.number_input("æ³¢æ•°ï¼ˆé–‹å§‹ï¼‰ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„:", min_value=-200, max_value=4800, value=pre_start_wavenum, step=100)
    end_wavenum = st.sidebar.number_input("æ³¢æ•°ï¼ˆçµ‚äº†ï¼‰ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„:", min_value=-200, max_value=4800, value=pre_end_wavenum, step=100)

    dssn_th = st.sidebar.number_input("ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„:", min_value=1, max_value=10000, value=1000, step=1)
    dssn_th = dssn_th/10000000
    
    # å¾®åˆ†ã®å¹³æ»‘åŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    num_firstDev = st.sidebar.number_input(
        "1æ¬¡å¾®åˆ†ã®å¹³æ»‘åŒ–ã®æ•°å€¤ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„:",
        min_value=1,
        max_value=35,
        value=13,
        step=2,
        key='unique_number_firstDev_key'
    )

    num_secondDev = st.sidebar.number_input(
        "2æ¬¡å¾®åˆ†ã®å¹³æ»‘åŒ–ã®æ•°å€¤ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„:",
        min_value=1,
        max_value=35,
        value=5,
        step=2,
        key='unique_number_secondDev_key'
    )
    
    num_threshold = st.sidebar.number_input(
        "é–¾å€¤ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„:",
        min_value=1,
        max_value=1000,
        value=10,
        step=10,
        key='unique_number_threshold_key'
    )

    # è¤‡æ•°ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
    uploaded_files = st.file_uploader("ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠã—ã¦ãã ã•ã„", accept_multiple_files=True)

    all_spectra = []  # ã™ã¹ã¦ã®ã‚¹ãƒšã‚¯ãƒˆãƒ«ã‚’æ ¼ç´ã™ã‚‹ãƒªã‚¹ãƒˆ
    all_bsremoval_spectra = []  # ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³è£œæ­£å¾Œã®ã‚¹ãƒšã‚¯ãƒˆãƒ«ã‚’æ ¼ç´ã™ã‚‹ãƒªã‚¹ãƒˆ
    all_averemoval_spectra = []  # ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³è£œæ­£å¾Œç§»å‹•å¹³å‡ã‚’è¡Œã£ãŸã‚¹ãƒšã‚¯ãƒˆãƒ«ã‚’æ ¼ç´ã™ã‚‹ãƒªã‚¹ãƒˆ
    file_labels = []  # å„ãƒ•ã‚¡ã‚¤ãƒ«åã®ãƒªã‚¹ãƒˆ
    
    if uploaded_files:
        
        # ã™ã¹ã¦ã®ãƒ•ã‚¡ã‚¤ãƒ«ã«å¯¾ã—ã¦å‡¦ç†
        for uploaded_file in uploaded_files:
            file_name = uploaded_file.name
            file_extension = file_name.split('.')[-1] if '.' in file_name else ''

            try:
                data = read_csv_file(uploaded_file, file_extension)
                file_type = detect_file_type(data)
                uploaded_file.seek(0)
                if file_type == "unknown":
                    st.error(f"{file_name}ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚¿ã‚¤ãƒ—ã‚’åˆ¤åˆ¥ã§ãã¾ã›ã‚“ã€‚")
                    continue

                # å„ãƒ•ã‚¡ã‚¤ãƒ«ã‚¿ã‚¤ãƒ—ã«å¯¾ã™ã‚‹å‡¦ç†
                if file_type == "wasatch":
                    st.write(f"ãƒ•ã‚¡ã‚¤ãƒ«ã‚¿ã‚¤ãƒ—: Wasatch ENLIGHTEN - {file_name}")
                    lambda_ex = 785
                    data = pd.read_csv(uploaded_file, encoding='shift-jis', skiprows=46)
                    pre_wavelength = np.array(data["Wavelength"].values)
                    pre_wavenum = (1e7 / lambda_ex) - (1e7 / pre_wavelength)
                    pre_spectra = np.array(data["Processed"].values)

                elif file_type == "ramaneye_old":
                    st.write(f"ãƒ•ã‚¡ã‚¤ãƒ«ã‚¿ã‚¤ãƒ—: RamanEye Data - {file_name}")
                    df_transposed = data.set_index("WaveNumber").T

                    # åˆ—åï¼ˆ"acrylic board" ãªã©ï¼‰ã‚’æ±ç”¨åŒ–
                    df_transposed.columns = ["intensity"]
                    
                    # æ³¢æ•°ã‚’floatã«å¤‰æ›ã—ã€ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«è¨­å®š
                    df_transposed.index = df_transposed.index.astype(float)
                    df_transposed = df_transposed.sort_index()
                    
                    # æ³¢æ•°ã¨å¼·åº¦ã‚’NumPyé…åˆ—ã¨ã—ã¦å–å¾—
                    pre_wavenum = df_transposed.index.to_numpy()
                    pre_spectra = df_transposed["intensity"].to_numpy()
                    
                    if pre_wavenum[0] > pre_wavenum[1]:
                        # pre_wavenum ã¨ pre_spectra ã‚’åè»¢
                        pre_wavenum = pre_wavenum[::-1]
                        pre_spectra = pre_spectra[::-1]
                        
                elif file_type == "ramaneye_new":
                    st.write(f"ãƒ•ã‚¡ã‚¤ãƒ«ã‚¿ã‚¤ãƒ—: RamanEye Data - {file_name}")
                    
                    data = pd.read_csv(uploaded_file, skiprows=9)
                    pre_wavenum = data["WaveNumber"]
                    pre_spectra = np.array(data.iloc[:, -1])  # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æŒ‡å®šã«åŸºã¥ãåˆ—ã‚’å–å¾—

                    if pre_wavenum[0] > pre_wavenum[1]:
                        # pre_wavenum ã¨ pre_spectra ã‚’åè»¢
                        pre_wavenum = pre_wavenum[::-1]
                        pre_spectra = pre_spectra[::-1]
                        
                elif file_type == "eagle":
                    st.write(f"ãƒ•ã‚¡ã‚¤ãƒ«ã‚¿ã‚¤ãƒ—: Eagle Data - {file_name}")
                    data_transposed = data.transpose()
                    header = data_transposed.iloc[:3]  # æœ€åˆã®3è¡Œ
                    reversed_data = data_transposed.iloc[3:].iloc[::-1]
                    data_transposed = pd.concat([header, reversed_data], ignore_index=True)
                    pre_wavenum = np.array(data_transposed.iloc[3:, 0])
                    pre_spectra = np.array(data_transposed.iloc[3:, 1])
                
                start_index = find_index(pre_wavenum, start_wavenum)
                end_index = find_index(pre_wavenum, end_wavenum)

                wavenum = np.array(pre_wavenum[start_index:end_index+1])
                spectra = np.array(pre_spectra[start_index:end_index+1])

                # Baseline and spike removal 
                spectra_spikerm = remove_outliers_and_interpolate(spectra)
                spectra_spikerm = np.asarray(spectra_spikerm, dtype=np.float64)
                mveAve_spectra = signal.medfilt(spectra_spikerm, savgol_wsize)
                lambda_ = 10e2
                baseline = airPLS(mveAve_spectra, dssn_th, lambda_, 2, 30)
                BSremoval_specta = spectra_spikerm - baseline
                BSremoval_specta_pos = BSremoval_specta + abs(np.minimum(spectra_spikerm, 0))  # è² å€¤ã‚’è£œæ­£

                # ç§»å‹•å¹³å‡å¾Œã®ã‚¹ãƒšã‚¯ãƒˆãƒ«
                Averemoval_specta = mveAve_spectra  - baseline
                Averemoval_specta_pos = Averemoval_specta + abs(np.minimum(mveAve_spectra, 0))  # è² å€¤ã‚’è£œæ­£

                # å„ã‚¹ãƒšã‚¯ãƒˆãƒ«ã‚’æ ¼ç´
                file_labels.append(file_name)  # ãƒ•ã‚¡ã‚¤ãƒ«åã‚’è¿½åŠ 
                all_spectra.append(spectra)
                all_bsremoval_spectra.append(BSremoval_specta_pos)
                all_averemoval_spectra.append(Averemoval_specta_pos)
                
            except Exception as e:
                st.error(f"{file_name}ã®å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
    
        # ã™ã¹ã¦ã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒå‡¦ç†ã•ã‚ŒãŸå¾Œã«é‡ã­ã¦ãƒ—ãƒ­ãƒƒãƒˆ
        fig, ax = plt.subplots(figsize=(10, 5))
        selected_colors = ['blue', 'red', 'green', 'orange', 'purple', 'brown', 'pink', 'cyan', 'yellow', 'black']
            
        # å…ƒã®ã‚¹ãƒšã‚¯ãƒˆãƒ«ã‚’é‡ã­ã¦ãƒ—ãƒ­ãƒƒãƒˆ
        for i, spectrum in enumerate(all_spectra):
            ax.plot(wavenum, spectrum, linestyle='-', color=selected_colors[i], label=f"{file_labels[i]}")
        ax.set_xlabel('WaveNumber / cm-1', fontsize=Fsize)
        ax.set_ylabel('Intensity / a.u.', fontsize=Fsize)
        ax.set_title('Raw Spectra', fontsize=Fsize)
        ax.legend(title="Spectra")
        st.pyplot(fig)
        
        export_df = pd.DataFrame({'WaveNumber': wavenum})
        for i, spectrum in enumerate(all_spectra):
            export_df[file_labels[i]] = spectrum
        
        csv_data = export_df.to_csv(index=False, encoding='utf-8-sig')
        
        st.download_button(
            label="Download Raw Spectra as CSV",
            data=csv_data,
            file_name='raw_spectra.csv',
            mime='text/csv'
        )
        
        # ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³è£œæ­£å¾Œ+ã‚¹ãƒ‘ã‚¤ã‚¯ä¿®æ­£å¾Œã®ã‚¹ãƒšã‚¯ãƒˆãƒ«ã‚’é‡ã­ã¦ãƒ—ãƒ­ãƒƒãƒˆ
        fig, ax = plt.subplots(figsize=(10, 5))        
        for i, spectrum in enumerate(all_bsremoval_spectra):
            ax.plot(wavenum, spectrum, linestyle='-', color=selected_colors[i], label=f"{file_labels[i]}")
        
        ax.set_xlabel('WaveNumber / cm-1', fontsize=Fsize)
        ax.set_ylabel('Intensity / a.u.', fontsize=Fsize)
        ax.set_title('Baseline Removed', fontsize=Fsize)
        st.pyplot(fig)
        
        export_df_bs = pd.DataFrame({'WaveNumber': wavenum})
        for i, spectrum in enumerate(all_bsremoval_spectra):
            export_df_bs[file_labels[i]] = spectrum
        
        csv_data_bs = export_df_bs.to_csv(index=False, encoding='utf-8-sig')
        
        st.download_button(
            label="Download Baseline Removed Spectra as CSV",
            data=csv_data_bs,
            file_name='baseline_removed_spectra.csv',
            mime='text/csv'
        )
        
        # ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³è£œæ­£å¾Œ+ã‚¹ãƒ‘ã‚¤ã‚¯ä¿®æ­£å¾Œ+ç§»å‹•å¹³å‡ã®ã‚¹ãƒšã‚¯ãƒˆãƒ«ã‚’é‡ã­ã¦ãƒ—ãƒ­ãƒƒãƒˆ
        fig, ax = plt.subplots(figsize=(10, 5))        
        for i, spectrum in enumerate(all_averemoval_spectra):
            ax.plot(wavenum, spectrum, linestyle='-', color=selected_colors[i], label=f"{file_labels[i]}")
        
        ax.set_xlabel('WaveNumber / cm-1', fontsize=Fsize)
        ax.set_ylabel('Intensity / a.u.', fontsize=Fsize)
        ax.set_title('Baseline Removed + Moving Average', fontsize=Fsize)
        st.pyplot(fig)
        
        # ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³è£œæ­£å¾Œ+ã‚¹ãƒ‘ã‚¤ã‚¯ä¿®æ­£å¾Œ+ç§»å‹•å¹³å‡ã‚¹ãƒšã‚¯ãƒˆãƒ«ã‚’CSVã§å‡ºåŠ›
        export_df_avg = pd.DataFrame({'WaveNumber': wavenum})
        for i, spectrum in enumerate(all_averemoval_spectra):
            export_df_avg[file_labels[i]] = spectrum
        
        csv_data_avg = export_df_avg.to_csv(index=False, encoding='utf-8-sig')
        
        st.download_button(
            label="Download Baseline Removed + Moving Average Spectra as CSV",
            data=csv_data_avg,
            file_name='baseline_removed_moving_avg_spectra.csv',
            mime='text/csv'
        )
        
        # ãƒ”ãƒ¼ã‚¯ä½ç½®ã®æ¤œå‡º
        if len(all_averemoval_spectra) > 0:  # ãƒ‡ãƒ¼ã‚¿ãŒå­˜åœ¨ã™ã‚‹å ´åˆã®ã¿å®Ÿè¡Œ
            firstDev_spectra = savitzky_golay(Averemoval_specta_pos, num_firstDev, savgol_order, 1)
            secondDev_spectra = savitzky_golay(Averemoval_specta_pos, num_secondDev, savgol_order, 2)
        
            peak_indices = np.where((firstDev_spectra[:-1] > 0) & (firstDev_spectra[1:] < 0) & 
                                      ((secondDev_spectra[:-1] / abs(np.min(secondDev_spectra[:-1]))) < -10/1000))[0]
            peaks = wavenum[peak_indices]
            
            peak_areas = []
            for peak_idx in peak_indices:
                start_idx, end_idx = find_peak_width(Averemoval_specta_pos, firstDev_spectra, peak_idx, window_size=20)
                area = find_peak_area(Averemoval_specta_pos, start_idx, end_idx)
                peak_areas.append(area)
            
            # Create a DataFrame to display peaks and their areas
            peak_data = {
                "ãƒ”ãƒ¼ã‚¯ä½ç½® (cmâ»Â¹)": peaks,
                "ãƒ”ãƒ¼ã‚¯é¢ç©": peak_areas
            }
            peak_df = pd.DataFrame(peak_data)
            
            # ãƒ”ãƒ¼ã‚¯ã®ä½ç½®ã‚’ãƒ—ãƒ­ãƒƒãƒˆ
            fig, ax = plt.subplots(figsize=(10, 5))
            ax.plot(wavenum, Averemoval_specta_pos, linestyle='-', color='b')
            for peak in peaks:
                ax.axvline(x=peak, color='r', linestyle='--', label=f'Peak at {peak}')
            ax.set_xlabel('WaveNumber / cm-1', fontsize=Fsize)
            ax.set_ylabel('Intensity / a.u.', fontsize=Fsize)
            ax.set_title('Peak Detection', fontsize=Fsize)
            st.pyplot(fig)

            # ãƒ”ãƒ¼ã‚¯ä½ç½®ã‚’è¡¨ç¤º
            st.write("ãƒ”ãƒ¼ã‚¯ä½ç½®:")
            st.table(peak_df)
             
        # Raman correlation table as a pandas DataFrame
        raman_data = {
            "ãƒ©ãƒãƒ³ã‚·ãƒ•ãƒˆ (cmâ»Â¹)": [
                "100â€“200", "150â€“450", "250â€“400", "290â€“330", "430â€“550", "450â€“550", "480â€“660", "500â€“700",
                "550â€“800", "630â€“790", "800â€“970", "1000â€“1250", "1300â€“1400", "1500â€“1600", "1600â€“1800", "2100â€“2250",
                "2800â€“3100", "3300â€“3500"
            ],
            "æŒ¯å‹•ãƒ¢ãƒ¼ãƒ‰ / åŒ–å­¦åŸº": [
                "æ ¼å­æŒ¯å‹• (Lattice vibrations)", "é‡‘å±-é…¸ç´ çµåˆ (Metal-O)", "C-C ã‚¢ãƒªãƒ•ã‚¡ãƒ†ã‚£ãƒƒã‚¯é–", "Se-Se", "S-S",
                "Si-O-Si", "C-I", "C-Br", "C-Cl", "C-S", "C-O-C", "C=S", "CHâ‚‚, CHâ‚ƒ (å¤‰è§’æŒ¯å‹•)",
                "èŠ³é¦™æ— C=C", "C=O (ã‚«ãƒ«ãƒœãƒ‹ãƒ«åŸº)", "Câ‰¡C, Câ‰¡N", "C-H (spÂ³, spÂ²)", "N-H, O-H"
            ],
            "å¼·åº¦": [
                "å¼·ã„ (Strong)", "ä¸­ã€œå¼±", "å¼·ã„", "å¼·ã„", "å¼·ã„", "å¼·ã„", "å¼·ã„", "å¼·ã„", "å¼·ã„", "ä¸­ã€œå¼·", "ä¸­ã€œå¼±", "å¼·ã„",
                "ä¸­ã€œå¼±", "å¼·ã„", "ä¸­ç¨‹åº¦", "ä¸­ã€œå¼·", "å¼·ã„", "ä¸­ç¨‹åº¦"
            ]
        }
        
        raman_df = pd.DataFrame(raman_data)
        
        # Display Raman correlation table
        st.subheader("ï¼ˆå‚è€ƒï¼‰ãƒ©ãƒãƒ³åˆ†å…‰ã®å¸°å±è¡¨")
        st.table(raman_df)

def multivariate_analysis_mode():
    """
    Multivariate analysis mode
    """
    st.header("ğŸ“Š å¤šå¤‰é‡è§£æãƒ„ãƒ¼ãƒ«")
    
    # Parameters in sidebar
    start_wavenum = st.sidebar.number_input("Start Wavenumber", value=400, min_value=0, key="mv_start")
    end_wavenum = st.sidebar.number_input("End Wavenumber", value=1800, min_value=start_wavenum + 1, key="mv_end")
    
    st.sidebar.subheader("ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³å‰Šé™¤")
    lambda_param = st.sidebar.number_input("ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼", value=1000, min_value=10, step=10, key="mv_lambda")
    porder = st.sidebar.selectbox("æŒ‡æ•°", options=[1, 2, 3], index=1, key="mv_porder")
    
    st.sidebar.subheader("å¤šå¤‰é‡è§£æ")
    n_components = st.sidebar.selectbox("ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆæ•°", options=[2, 3, 4, 5], index=0, key="mv_components")
    
    # File upload
    st.header("ğŸ“ ãƒ‡ãƒ¼ã‚¿ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰")
    uploaded_files = st.file_uploader(
        "Choose CSV files",
        type=['csv', 'txt'],
        accept_multiple_files=True,
        help="Upload multiple CSV files with spectral data. Files should have Wavenumber, Raw, and Processed columns starting from row 46.",
        key="mv_uploader"
    )
    
    if uploaded_files:
        st.success(f"Uploaded {len(uploaded_files)} files")
        
        # Show uploaded file names
        st.write("**Uploaded files:**")
        for file in uploaded_files:
            st.write(f"- {file.name}")
        
        # Process button
        if st.button("ğŸ”„ ãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ­ã‚»ã‚¹å®ŸåŠ¹", type="primary", key="mv_process"):
            with st.spinner("ã‚¹ãƒšã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’è§£æã—ã¦ã„ã¾ã™..."):
                
                # Load and process data
                grouped_data, all_baseline_spectra, all_labels, wavenumber = load_and_process_data_for_multivariate(
                    uploaded_files, start_wavenum, end_wavenum, lambda_param, porder
                )
                
                if len(all_baseline_spectra) > 0:
                    # Store data in session state
                    st.session_state.mv_grouped_data = grouped_data
                    st.session_state.mv_all_baseline_spectra = all_baseline_spectra
                    st.session_state.mv_all_labels = all_labels
                    st.session_state.mv_wavenumber = wavenumber
                    
                    st.success("ãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ­ã‚»ã‚¹å®Œäº†!")
                else:
                    st.error("ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã«æœ‰åŠ¹ãªãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
    
    # Display results if data is processed
    if hasattr(st.session_state, 'mv_grouped_data') and st.session_state.mv_grouped_data:
        
        st.header("ğŸ“ˆ ã‚¹ãƒšã‚¯ãƒˆãƒ«ãƒ—ãƒ­ãƒƒãƒˆ")
        
        # Plot original spectra
        fig_spectra = plot_spectra_matplotlib(st.session_state.mv_grouped_data)
        st.pyplot(fig_spectra)
        
        # Display data summary
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("Total Spectra", len(st.session_state.mv_all_baseline_spectra))
        with col2:
            st.metric("Number of Groups", len(st.session_state.mv_grouped_data))
        with col3:
            st.metric("Data Points per Spectrum", len(st.session_state.mv_wavenumber))
        
        # Analysis
        st.header("ğŸ”¬ å¤šå¤‰é‡è§£æ")
        
        if st.button("ğŸš€ å¤šå¤‰é‡è§£æå®ŸåŠ¹", type="primary", key="mv_analyze"):
            with st.spinner("å¤šå¤‰é‡è§£æã‚’è¡Œã£ã¦ã„ã¾ã™..."):
                
                # Perform
                W, H, nmf_model = perform_nmf_analysis(st.session_state.mv_all_baseline_spectra, n_components)
                
                # Store NMF results
                st.session_state.mv_W = W
                st.session_state.mv_H = H
                st.session_state.mv_nmf_model = nmf_model
                
                # Calculate reconstruction error
                reconstruction_error = nmf_model.reconstruction_err_
                st.session_state.mv_reconstruction_error = reconstruction_error
                
                st.success(f"è§£æå®Œäº†! Reconstruction error: {reconstruction_error:.4f}")
        
        # Display NMF results if available
        if hasattr(st.session_state, 'mv_W') and hasattr(st.session_state, 'mv_H'):
            
            # Display reconstruction error
            st.info(f"**Reconstruction Error:** {st.session_state.mv_reconstruction_error:.4f}")
            
            # Plot NMF components
            st.subheader("ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚¹ãƒšã‚¯ãƒˆãƒ«")
            fig_components = plot_nmf_components(st.session_state.mv_H, st.session_state.mv_wavenumber)
            st.pyplot(fig_components)
            
            # Plot NMF scores
            st.subheader("ã‚¹ã‚³ã‚¢ãƒ—ãƒ­ãƒƒãƒˆ")
            group_names = list(st.session_state.mv_grouped_data.keys())
            fig_scores = plot_nmf_scores(st.session_state.mv_W, st.session_state.mv_all_labels, group_names)
            if fig_scores:
                st.pyplot(fig_scores)
            
            # Display contribution ratios
            st.subheader("Component Contribution Ratios")
            contribution_ratios = np.sum(st.session_state.mv_W, axis=0)
            contribution_ratios = contribution_ratios / np.sum(contribution_ratios)
            
            contrib_df = pd.DataFrame({
                'Component': [f"Component {i+1}" for i in range(len(contribution_ratios))],
                'Contribution Ratio': contribution_ratios,
                'Percentage': contribution_ratios * 100
            })
            
            st.dataframe(contrib_df, use_container_width=True)
            
            # Bar chart of contributions
            fig_contrib = plot_contribution_ratios(contribution_ratios)
            st.pyplot(fig_contrib)
            
            # Download results
            st.subheader("ğŸ“¥ ãƒ‡ãƒ¼ã‚¿ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰")
            
            col1, col2 = st.columns(2)
            
            with col1:
                # Download concentration matrix
                W_df = pd.DataFrame(st.session_state.mv_W, 
                                  columns=[f"Component_{i+1}" for i in range(st.session_state.mv_W.shape[1])])
                W_df['Sample'] = st.session_state.mv_all_labels
                csv_W = W_df.to_csv(index=False)
                st.download_button(
                    label="ğŸ“Š Download Concentration Matrix",
                    data=csv_W,
                    file_name="nmf_concentration_matrix.csv",
                    mime="text/csv",
                    key="mv_download_W"
                )
            
            with col2:
                # Download spectral components
                H_df = pd.DataFrame(st.session_state.mv_H.T, 
                                  columns=[f"Component_{i+1}" for i in range(st.session_state.mv_H.shape[0])])
                H_df['Wavenumber'] = st.session_state.mv_wavenumber
                csv_H = H_df.to_csv(index=False)
                st.download_button(
                    label="ğŸ“ˆ Download Spectral Components",
                    data=csv_H,
                    file_name="nmf_spectral_components.csv",
                    mime="text/csv",
                    key="mv_download_H"
                )
            
            # Advanced analysis section
            st.subheader("ğŸ” Advanced Analysis")
            
            # Show individual sample information
            if st.checkbox("Show Individual Sample Information", key="mv_sample_info"):
                sample_info_df = pd.DataFrame({
                    'Sample Index': range(len(st.session_state.mv_all_labels)),
                    'Group': st.session_state.mv_all_labels,
                    **{f'Component_{i+1}': st.session_state.mv_W[:, i] for i in range(st.session_state.mv_W.shape[1])}
                })
                st.dataframe(sample_info_df, use_container_width=True)

def main():
    st.set_page_config(page_title="çµ±åˆãƒ©ãƒãƒ³ã‚¹ãƒšã‚¯ãƒˆãƒ«è§£æãƒ„ãƒ¼ãƒ«", page_icon="ğŸ“Š", layout="wide")
    
    st.title("ğŸ“Š çµ±åˆãƒ©ãƒãƒ³ã‚¹ãƒšã‚¯ãƒˆãƒ«è§£æãƒ„ãƒ¼ãƒ«")
    
    # Mode selection in sidebar
    st.sidebar.header("ğŸ”§ è§£æãƒ¢ãƒ¼ãƒ‰é¸æŠ")
    analysis_mode = st.sidebar.selectbox(
        "è§£æãƒ¢ãƒ¼ãƒ‰ã‚’é¸æŠã—ã¦ãã ã•ã„:",
        ["ã‚¹ãƒšã‚¯ãƒˆãƒ«è§£æ", "å¤šå¤‰é‡è§£æ"],
        key="mode_selector"
    )
    
    st.sidebar.markdown("---")
    st.sidebar.header("ğŸ“‹ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®š")
    
    if analysis_mode == "ã‚¹ãƒšã‚¯ãƒˆãƒ«è§£æ":
        spectrum_analysis_mode()
    else:
        multivariate_analysis_mode()
    
    # Instructions
    st.sidebar.markdown("---")
    st.sidebar.subheader("ğŸ“‹ ä½¿ç”¨æ–¹æ³•")
    
    if analysis_mode == "ã‚¹ãƒšã‚¯ãƒˆãƒ«è§£æ":
        st.sidebar.markdown("""
        **ã‚¹ãƒšã‚¯ãƒˆãƒ«è§£æãƒ¢ãƒ¼ãƒ‰:**
        1. è§£æã—ãŸã„CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
        2. ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’èª¿æ•´
        3. ã‚¹ãƒšã‚¯ãƒˆãƒ«ã®è¡¨ç¤ºã¨è§£æçµæœã‚’ç¢ºèª
        4. ãƒ”ãƒ¼ã‚¯æ¤œå‡ºçµæœã¨ãƒ©ãƒãƒ³å¸°å±è¡¨ã‚’å‚ç…§
        5. çµæœã‚’CSVãƒ•ã‚¡ã‚¤ãƒ«ã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
        """)
    else:
        st.sidebar.markdown("""
        **å¤šå¤‰é‡è§£æãƒ¢ãƒ¼ãƒ‰:**
        1. è¤‡æ•°ã®CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
        2. ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’èª¿æ•´
        3. ã€Œãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ­ã‚»ã‚¹å®ŸåŠ¹ã€ã‚’ã‚¯ãƒªãƒƒã‚¯
        4. ã€Œå¤šå¤‰é‡è§£æå®ŸåŠ¹ã€ã‚’ã‚¯ãƒªãƒƒã‚¯
        5. è§£æçµæœã‚’ç¢ºèªãƒ»ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
        
        **ãƒ•ã‚¡ã‚¤ãƒ«å‘½åè¦å‰‡:**
        GroupName_Number.csv
        """)

if __name__ == "__main__":
    main()
    
